<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home | UFHCC BCB-SR Bioinformatics Wiki</title><link>https://ufhcc-bcbsr.github.io/wiki/</link><atom:link href="https://ufhcc-bcbsr.github.io/wiki/index.xml" rel="self" type="application/rss+xml"/><description>Home</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><image><url>https://ufhcc-bcbsr.github.io/wiki/media/icon_hu_bb893f6dc6eaf6fe.png</url><title>Home</title><link>https://ufhcc-bcbsr.github.io/wiki/</link></image><item><title>Code Availability Statment</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/publishing/code-availability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/publishing/code-availability/</guid><description>&lt;p>Journals (often) have strict yet confusing requirements for how they want manuscript figures to look. Here are some tips for how to handle this. [Using R]&lt;/p>
&lt;h2 id="getting-the-right-sizeresolution">Getting the right size/resolution&lt;/h2>
&lt;p>Journals often have size limits and recommended resolution for figures (e.g. no more than 6 in wide and 7 in tall, 600dpi resolution). Good news: if you use the ggplot2 package to make plots, you can then save your figures using &lt;a href="http://ggplot2.tidyverse.org/reference/ggsave.html" target="_blank" rel="noopener">ggsave()&lt;/a>! There are options for width, height, and dpi. And since the resulting file will almost certainly be larger than the journal&amp;rsquo;s size limit, use the argument compression=&amp;lsquo;lzw&amp;rsquo; to make it smaller.&lt;/p>
&lt;h2 id="multi-part-figures">Multi-part figures&lt;/h2>
&lt;p>I have found the package &lt;a href="https://cran.r-project.org/web/packages/multipanelfigure/multipanelfigure.pdf" target="_blank" rel="noopener">multipanelfigure&lt;/a> to be very useful. It allows you to make a multi-part figure by putting together any kind of objects: ggplot figures, base R figures, pngs, whatever. There is a fairly easy to understand example on their &lt;a href="https://github.com/cran/multipanelfigure" target="_blank" rel="noopener">GitHub repo&lt;/a>, and I used it in &lt;a href="https://github.com/emchristensen/Extreme-events-LDA/blob/master/rodent_LDA_analysis.r" target="_blank" rel="noopener">line 188 of this script&lt;/a>. The general idea is you create a grid object with the function multi_panel_figure(), in which you specify things like size of columns/rows:&lt;/p>
&lt;p>&lt;code>grid_object &amp;lt;- multi_panel_figure(width = c(50,50), height = c(50,50))&lt;/code>&lt;/p>
&lt;p>and then fill the grid one cell at a time&lt;/p>
&lt;p>&lt;code>grid_object %&amp;lt;&amp;gt;% fill_panel(ggplot_figure, row = 1, column = 1)&lt;/code>&lt;/p>
&lt;p>&lt;code>grid_object %&amp;lt;&amp;gt;% fill_panel(png_path, row = 1, column = 2)&lt;/code>&lt;/p>
&lt;p>You can then use the function save_multi_panel_figure() which is similar to ggsave(), and again specify dpi and compression. Getting it to the right size is harder, I ended up playing around with column and row sizes until it fit.&lt;/p></description></item><item><title>Data Availability Statement</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/publishing/data-availability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/publishing/data-availability/</guid><description>&lt;p>Journals (often) have strict yet confusing requirements for how they want manuscript figures to look. Here are some tips for how to handle this. [Using R]&lt;/p>
&lt;h2 id="getting-the-right-sizeresolution">Getting the right size/resolution&lt;/h2>
&lt;p>Journals often have size limits and recommended resolution for figures (e.g. no more than 6 in wide and 7 in tall, 600dpi resolution). Good news: if you use the ggplot2 package to make plots, you can then save your figures using &lt;a href="http://ggplot2.tidyverse.org/reference/ggsave.html" target="_blank" rel="noopener">ggsave()&lt;/a>! There are options for width, height, and dpi. And since the resulting file will almost certainly be larger than the journal&amp;rsquo;s size limit, use the argument compression=&amp;lsquo;lzw&amp;rsquo; to make it smaller.&lt;/p>
&lt;h2 id="multi-part-figures">Multi-part figures&lt;/h2>
&lt;p>I have found the package &lt;a href="https://cran.r-project.org/web/packages/multipanelfigure/multipanelfigure.pdf" target="_blank" rel="noopener">multipanelfigure&lt;/a> to be very useful. It allows you to make a multi-part figure by putting together any kind of objects: ggplot figures, base R figures, pngs, whatever. There is a fairly easy to understand example on their &lt;a href="https://github.com/cran/multipanelfigure" target="_blank" rel="noopener">GitHub repo&lt;/a>, and I used it in &lt;a href="https://github.com/emchristensen/Extreme-events-LDA/blob/master/rodent_LDA_analysis.r" target="_blank" rel="noopener">line 188 of this script&lt;/a>. The general idea is you create a grid object with the function multi_panel_figure(), in which you specify things like size of columns/rows:&lt;/p>
&lt;p>&lt;code>grid_object &amp;lt;- multi_panel_figure(width = c(50,50), height = c(50,50))&lt;/code>&lt;/p>
&lt;p>and then fill the grid one cell at a time&lt;/p>
&lt;p>&lt;code>grid_object %&amp;lt;&amp;gt;% fill_panel(ggplot_figure, row = 1, column = 1)&lt;/code>&lt;/p>
&lt;p>&lt;code>grid_object %&amp;lt;&amp;gt;% fill_panel(png_path, row = 1, column = 2)&lt;/code>&lt;/p>
&lt;p>You can then use the function save_multi_panel_figure() which is similar to ggsave(), and again specify dpi and compression. Getting it to the right size is harder, I ended up playing around with column and row sizes until it fit.&lt;/p></description></item><item><title>Publication Quality Figures</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/publication-figures/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/publication-figures/</guid><description>&lt;p>Journals (often) have strict yet confusing requirements for how they want manuscript figures to look. Here are some tips for how to handle this. [Using R]&lt;/p>
&lt;h2 id="getting-the-right-sizeresolution">Getting the right size/resolution&lt;/h2>
&lt;p>Journals often have size limits and recommended resolution for figures (e.g. no more than 6 in wide and 7 in tall, 600dpi resolution). Good news: if you use the ggplot2 package to make plots, you can then save your figures using &lt;a href="http://ggplot2.tidyverse.org/reference/ggsave.html" target="_blank" rel="noopener">ggsave()&lt;/a>! There are options for width, height, and dpi. And since the resulting file will almost certainly be larger than the journal&amp;rsquo;s size limit, use the argument compression=&amp;lsquo;lzw&amp;rsquo; to make it smaller.&lt;/p>
&lt;h2 id="multi-part-figures">Multi-part figures&lt;/h2>
&lt;p>I have found the package &lt;a href="https://cran.r-project.org/web/packages/multipanelfigure/multipanelfigure.pdf" target="_blank" rel="noopener">multipanelfigure&lt;/a> to be very useful. It allows you to make a multi-part figure by putting together any kind of objects: ggplot figures, base R figures, pngs, whatever. There is a fairly easy to understand example on their &lt;a href="https://github.com/cran/multipanelfigure" target="_blank" rel="noopener">GitHub repo&lt;/a>, and I used it in &lt;a href="https://github.com/emchristensen/Extreme-events-LDA/blob/master/rodent_LDA_analysis.r" target="_blank" rel="noopener">line 188 of this script&lt;/a>. The general idea is you create a grid object with the function multi_panel_figure(), in which you specify things like size of columns/rows:&lt;/p>
&lt;p>&lt;code>grid_object &amp;lt;- multi_panel_figure(width = c(50,50), height = c(50,50))&lt;/code>&lt;/p>
&lt;p>and then fill the grid one cell at a time&lt;/p>
&lt;p>&lt;code>grid_object %&amp;lt;&amp;gt;% fill_panel(ggplot_figure, row = 1, column = 1)&lt;/code>&lt;/p>
&lt;p>&lt;code>grid_object %&amp;lt;&amp;gt;% fill_panel(png_path, row = 1, column = 2)&lt;/code>&lt;/p>
&lt;p>You can then use the function save_multi_panel_figure() which is similar to ggsave(), and again specify dpi and compression. Getting it to the right size is harder, I ended up playing around with column and row sizes until it fit.&lt;/p></description></item><item><title>Publication Quality Figures</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/publishing/publication-figures/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/publishing/publication-figures/</guid><description>&lt;p>Journals (often) have strict yet confusing requirements for how they want manuscript figures to look. Here are some tips for how to handle this. [Using R]&lt;/p>
&lt;h2 id="getting-the-right-sizeresolution">Getting the right size/resolution&lt;/h2>
&lt;p>Journals often have size limits and recommended resolution for figures (e.g. no more than 6 in wide and 7 in tall, 600dpi resolution). Good news: if you use the ggplot2 package to make plots, you can then save your figures using &lt;a href="http://ggplot2.tidyverse.org/reference/ggsave.html" target="_blank" rel="noopener">ggsave()&lt;/a>! There are options for width, height, and dpi. And since the resulting file will almost certainly be larger than the journal&amp;rsquo;s size limit, use the argument compression=&amp;lsquo;lzw&amp;rsquo; to make it smaller.&lt;/p>
&lt;h2 id="multi-part-figures">Multi-part figures&lt;/h2>
&lt;p>I have found the package &lt;a href="https://cran.r-project.org/web/packages/multipanelfigure/multipanelfigure.pdf" target="_blank" rel="noopener">multipanelfigure&lt;/a> to be very useful. It allows you to make a multi-part figure by putting together any kind of objects: ggplot figures, base R figures, pngs, whatever. There is a fairly easy to understand example on their &lt;a href="https://github.com/cran/multipanelfigure" target="_blank" rel="noopener">GitHub repo&lt;/a>, and I used it in &lt;a href="https://github.com/emchristensen/Extreme-events-LDA/blob/master/rodent_LDA_analysis.r" target="_blank" rel="noopener">line 188 of this script&lt;/a>. The general idea is you create a grid object with the function multi_panel_figure(), in which you specify things like size of columns/rows:&lt;/p>
&lt;p>&lt;code>grid_object &amp;lt;- multi_panel_figure(width = c(50,50), height = c(50,50))&lt;/code>&lt;/p>
&lt;p>and then fill the grid one cell at a time&lt;/p>
&lt;p>&lt;code>grid_object %&amp;lt;&amp;gt;% fill_panel(ggplot_figure, row = 1, column = 1)&lt;/code>&lt;/p>
&lt;p>&lt;code>grid_object %&amp;lt;&amp;gt;% fill_panel(png_path, row = 1, column = 2)&lt;/code>&lt;/p>
&lt;p>You can then use the function save_multi_panel_figure() which is similar to ggsave(), and again specify dpi and compression. Getting it to the right size is harder, I ended up playing around with column and row sizes until it fit.&lt;/p></description></item><item><title>RNA-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/analysis-pipelines/rna-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/analysis-pipelines/rna-seq/</guid><description>&lt;h2 id="who-we-are">Who We Are&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://directory.ufhealth.org/brant-jason/" target="_blank" rel="noopener">Dr. Jason O. Brant, Unit Leader&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://directory.ufhealth.org/kates-heather/" target="_blank" rel="noopener">Dr. Heather R. Kates, Bioinformatics Analyst III&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://directory.ufhealth.org/shirlekar-kalyanee" target="_blank" rel="noopener">Kalyanee Shirlekar, MS, Bioinformatics Analyst III&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="why-uf-health-cancer-center-bcb-sr">Why UF Health Cancer Center BCB-SR?&lt;/h2>
&lt;p>Each of us at BCB-SR Bioinformatics is driven by the mission to bridge the gap between data and discovery. We are all trained as biologists as well as bioinformaticians, and take a biology-first view of bioinformatics analysis.&lt;/p>
&lt;h2 id="careers">Careers&lt;/h2>
&lt;p>Want to work as a bioformatician at UF? We&amp;rsquo;d love to hear from you to support your career development. You can search for jobs at &lt;a href="https://explore.jobs.ufl.edu/en-us/listing/" target="_blank" rel="noopener">Careers at UF&lt;/a> using broad search terms including &amp;ldquo;data&amp;rdquo; and &amp;ldquo;analyst&amp;rdquo;, as not all programs realize that what they are looking for is a &lt;em>bioinformatician&lt;/em>!&lt;/p></description></item><item><title>RNA-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/differential-analysis/rna-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/differential-analysis/rna-seq/</guid><description>&lt;h2 id="who-we-are">Who We Are&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://directory.ufhealth.org/brant-jason/" target="_blank" rel="noopener">Dr. Jason O. Brant, Unit Leader&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://directory.ufhealth.org/kates-heather/" target="_blank" rel="noopener">Dr. Heather R. Kates, Bioinformatics Analyst III&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://directory.ufhealth.org/shirlekar-kalyanee" target="_blank" rel="noopener">Kalyanee Shirlekar, MS, Bioinformatics Analyst III&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="why-uf-health-cancer-center-bcb-sr">Why UF Health Cancer Center BCB-SR?&lt;/h2>
&lt;p>Each of us at BCB-SR Bioinformatics is driven by the mission to bridge the gap between data and discovery. We are all trained as biologists as well as bioinformaticians, and take a biology-first view of bioinformatics analysis.&lt;/p>
&lt;h2 id="careers">Careers&lt;/h2>
&lt;p>Want to work as a bioformatician at UF? We&amp;rsquo;d love to hear from you to support your career development. You can search for jobs at &lt;a href="https://explore.jobs.ufl.edu/en-us/listing/" target="_blank" rel="noopener">Careers at UF&lt;/a> using broad search terms including &amp;ldquo;data&amp;rdquo; and &amp;ldquo;analyst&amp;rdquo;, as not all programs realize that what they are looking for is a &lt;em>bioinformatician&lt;/em>!&lt;/p></description></item><item><title>RNA-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/experimental-design/rna-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/experimental-design/rna-seq/</guid><description>&lt;h2 id="who-we-are">Who We Are&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://directory.ufhealth.org/brant-jason/" target="_blank" rel="noopener">Dr. Jason O. Brant, Unit Leader&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://directory.ufhealth.org/kates-heather/" target="_blank" rel="noopener">Dr. Heather R. Kates, Bioinformatics Analyst III&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://directory.ufhealth.org/shirlekar-kalyanee" target="_blank" rel="noopener">Kalyanee Shirlekar, MS, Bioinformatics Analyst III&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="why-uf-health-cancer-center-bcb-sr">Why UF Health Cancer Center BCB-SR?&lt;/h2>
&lt;p>Each of us at BCB-SR Bioinformatics is driven by the mission to bridge the gap between data and discovery. We are all trained as biologists as well as bioinformaticians, and take a biology-first view of bioinformatics analysis.&lt;/p>
&lt;h2 id="careers">Careers&lt;/h2>
&lt;p>Want to work as a bioformatician at UF? We&amp;rsquo;d love to hear from you to support your career development. You can search for jobs at &lt;a href="https://explore.jobs.ufl.edu/en-us/listing/" target="_blank" rel="noopener">Careers at UF&lt;/a> using broad search terms including &amp;ldquo;data&amp;rdquo; and &amp;ldquo;analyst&amp;rdquo;, as not all programs realize that what they are looking for is a &lt;em>bioinformatician&lt;/em>!&lt;/p></description></item><item><title>ATAC-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/analysis-pipelines/atac-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/analysis-pipelines/atac-seq/</guid><description>&lt;h2 id="submit-a-support-request-form">Submit a Support Request Form&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://cancer.ufl.edu/research/shared-resources/biostatistics-computational-biology-shared-resource/biostatistics-shared-resource-support-request-form/" target="_blank" rel="noopener">Online Support Requet Form&lt;/a>:&lt;/li>
&lt;/ul>
&lt;p>Fill out the support request form as best you can. The important thing is to get connected with us, we&amp;rsquo;ll learn much more about your project later.&lt;/p>
&lt;h2 id="schedule-a-meeting-with-us">Schedule a Meeting With Us&lt;/h2>
&lt;p>Once we receive your support request, we&amp;rsquo;ll send you an email to schedule a meeting. Initial meetings are usually about an hour long, during which we want to understand your project and the expectations of a future collaboration.&lt;/p>
&lt;h2 id="how-does-the-funding-work">How Does the Funding Work?&lt;/h2>
&lt;p>The BCB-SR is a group of bioinformatics scientists whose salaries are primarily funded by effort on the grants on which they collaborate. Because of this, the ideal scenario is that you identify the need for bioinformatics collaboration &lt;strong>before&lt;/strong> your grant is submitted so that there will be an appropriate allocation of funding. If this is not the case, we will estimate our effort given the project&amp;rsquo;s needs and determine whether the project has sufficient funding available to support bioinformatics collaboration.&lt;/p></description></item><item><title>ATAC-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/differential-analysis/atac-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/differential-analysis/atac-seq/</guid><description>&lt;h2 id="submit-a-support-request-form">Submit a Support Request Form&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://cancer.ufl.edu/research/shared-resources/biostatistics-computational-biology-shared-resource/biostatistics-shared-resource-support-request-form/" target="_blank" rel="noopener">Online Support Requet Form&lt;/a>:&lt;/li>
&lt;/ul>
&lt;p>Fill out the support request form as best you can. The important thing is to get connected with us, we&amp;rsquo;ll learn much more about your project later.&lt;/p>
&lt;h2 id="schedule-a-meeting-with-us">Schedule a Meeting With Us&lt;/h2>
&lt;p>Once we receive your support request, we&amp;rsquo;ll send you an email to schedule a meeting. Initial meetings are usually about an hour long, during which we want to understand your project and the expectations of a future collaboration.&lt;/p>
&lt;h2 id="how-does-the-funding-work">How Does the Funding Work?&lt;/h2>
&lt;p>The BCB-SR is a group of bioinformatics scientists whose salaries are primarily funded by effort on the grants on which they collaborate. Because of this, the ideal scenario is that you identify the need for bioinformatics collaboration &lt;strong>before&lt;/strong> your grant is submitted so that there will be an appropriate allocation of funding. If this is not the case, we will estimate our effort given the project&amp;rsquo;s needs and determine whether the project has sufficient funding available to support bioinformatics collaboration.&lt;/p></description></item><item><title>ATAC-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/experimental-design/atac-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/experimental-design/atac-seq/</guid><description>&lt;h2 id="submit-a-support-request-form">Submit a Support Request Form&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://cancer.ufl.edu/research/shared-resources/biostatistics-computational-biology-shared-resource/biostatistics-shared-resource-support-request-form/" target="_blank" rel="noopener">Online Support Requet Form&lt;/a>:&lt;/li>
&lt;/ul>
&lt;p>Fill out the support request form as best you can. The important thing is to get connected with us, we&amp;rsquo;ll learn much more about your project later.&lt;/p>
&lt;h2 id="schedule-a-meeting-with-us">Schedule a Meeting With Us&lt;/h2>
&lt;p>Once we receive your support request, we&amp;rsquo;ll send you an email to schedule a meeting. Initial meetings are usually about an hour long, during which we want to understand your project and the expectations of a future collaboration.&lt;/p>
&lt;h2 id="how-does-the-funding-work">How Does the Funding Work?&lt;/h2>
&lt;p>The BCB-SR is a group of bioinformatics scientists whose salaries are primarily funded by effort on the grants on which they collaborate. Because of this, the ideal scenario is that you identify the need for bioinformatics collaboration &lt;strong>before&lt;/strong> your grant is submitted so that there will be an appropriate allocation of funding. If this is not the case, we will estimate our effort given the project&amp;rsquo;s needs and determine whether the project has sufficient funding available to support bioinformatics collaboration.&lt;/p></description></item><item><title>Submitting a manuscript</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/manuscript-submission/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/manuscript-submission/</guid><description>&lt;p>Congrats! You&amp;rsquo;re getting ready to submit a manuscript for review at a journal. This guide will walk you through the various steps in the process, however, it assumes you&amp;rsquo;ve already decided what journal to submit to. Right now, there is no wiki page on that, so talk with Ethan or Morgan or one of the postdocs/staff for advice.&lt;/p>
&lt;h2 id="manuscript-formatting">Manuscript Formatting&lt;/h2>
&lt;p>Go to the journal&amp;rsquo;s website, and they should have a section called &amp;ldquo;For Authors&amp;rdquo;, &amp;ldquo;Author Instructions&amp;rdquo;, &amp;ldquo;Guide to Authors&amp;rdquo; or something along those lines. They should have detailed guidelines for citation style, section order, cover letter requirements, submission website link, etc. They will also, hopefully in the same location, have a description of the focus of the journal, which your manuscript should fall within.&lt;/p>
&lt;p>The journal may have document templates (most commonly for Microsoft Word or LaTeX). If you are working in LaTeX, you may want to consider &lt;a href="https://www.overleaf.com/" target="_blank" rel="noopener">Overleaf&lt;/a>, which has some built-in templates for journals. If you are working in Rmarkdown, the &lt;a href="https://github.com/rstudio/rticles" target="_blank" rel="noopener">&lt;code>rticles&lt;/code>&lt;/a> package also has some common templates.&lt;/p>
&lt;p>Using a citation manager that can format references using Citation Style Language (CSL) is highly recommended. Many styles can be found or modified from existing ones at &lt;a href="https://www.zotero.org/styles" target="_blank" rel="noopener">Zotero&amp;rsquo;s CSL list&lt;/a>. Many Rmarkdown outputs accept CSL files to format references.&lt;/p>
&lt;p>&lt;a href="https://ufhcc-bcbsr.github.io/wiki/docs/publishing/headings">Use real headings&lt;/a>.&lt;/p>
&lt;h2 id="recommending-reviewers--editors">Recommending reviewers &amp;amp; editors&lt;/h2>
&lt;p>Journals typically ask you to recommend 3 or more people who could potentially review your manuscript. Those people may or may not end up being reviewers, and they might not even be asked, but providing a good list will help increase the chance that the review process starts quickly and that you get good feedback from relevant reviewers.&lt;/p>
&lt;p>It&amp;rsquo;s probably most important to recommend scientists who have similar philosophies to yours (e.g., if the paper has a macroecological approach, don&amp;rsquo;t choose someone who thinks fieldwork is the only way to do ecology).&lt;/p>
&lt;p>Don&amp;rsquo;t recommend anyone who could have a conflict of interest. The idea is that people who would be biased by personal or institutional relationships to the authors should review the paper. Typically this is considered to include anyone at your current institution and anyone who&amp;rsquo;s been a (close) coauthor on a paper with any of the current manuscript&amp;rsquo;s coauthors in the last 5 - 10 years or so. You should ask coauthors to check the list of reviewer recommendations to conflicts before submitting.&lt;/p>
&lt;p>Some journals will also ask for editor recommendations. There should be a page on the journal&amp;rsquo;s website with the editorial board, choose 1 - 2 associate editors that seem most appropriate, based on the manuscript topic.&lt;/p>
&lt;p>Some journals will also allow you to designate reviewers with &amp;ldquo;conflicts&amp;rdquo; or something similar. This is not a list of people who have a conflict of interest as described above. It is an opportunity to designate people that you don&amp;rsquo;t want to review the paper because they are predisposed to review it negatively (e.g., they are directly competing with you in the research space, or have known objections to certain topics/approaches). We basically never list people, but if you are concerned about someone chat with your advisor.&lt;/p>
&lt;h2 id="cover-letter">Cover letter&lt;/h2>
&lt;p>All cover letters consist of:&lt;/p>
&lt;ul>
&lt;li>a business header (either the name of Editor in Chief &amp;amp; their professional address or your name and professional address) (if you forget this part it is not a problem and choosing one header or the other won&amp;rsquo;t raise any eyebrows though to the editor is more traditional)&lt;/li>
&lt;li>a description consisting of the manuscript title, authors, and what &amp;rsquo;type&amp;rsquo; of article it is being submitted as for consideration (e.g., Report, Methods, Ideas and Concepts, Article, this varies from journal to journal and some journals do not have different types of article)&lt;/li>
&lt;li>a brief description of the manuscript (typically 1 paragraph). For many journals, this description should convey either implicitly or explicitly why this article is of interest to the specific readership of this journal. If going to a general ecology journal like Ecology/Ecology Letters/AmNat then this description should focus on the aspects of your work that a general readership would find interesting. If going to GEB then focus on the general interest and global nature of the study. etc etc. If you are going for one of the journals whose mission is to publish what they think is the most influential research (Science, Nature, PLoS Biology, PNAS, eLife) then you need to emphasize the novelty and importance of the work and more than 1 paragraph may be acceptable (see the Riemer example below).&lt;/li>
&lt;li>Any assurances or statements the journal requires (i.e. the work is the authors&amp;rsquo; own, it is not submitted elsewhere, etc).&lt;/li>
&lt;/ul>
&lt;p>Your letter should not be longer than 1 page, including the header material and your &amp;lsquo;signature&amp;rsquo; at the bottom (don&amp;rsquo;t need to literally sign it).&lt;/p>
&lt;p>Here are some examples of cover letters&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/sdtaylor/phenology_dataset_study/blob/master/manuscript/cover_letter.txt" target="_blank" rel="noopener">Cover letter&lt;/a> for &lt;a href="https://doi.org/10.1002/ecy.2568" target="_blank" rel="noopener">Taylor et al. 2018&lt;/a>&lt;/li>
&lt;li>&lt;a href="../cover-letter-first-submission.pdf">Cover letter&lt;/a> for &lt;a href="https://doi.org/10.7554/eLife.27166" target="_blank" rel="noopener">Riemer et al. 2018&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="data-and-code-archiving">Data and Code Archiving&lt;/h2>
&lt;p>Recommendation: Archive your code on &lt;a href="https://zenodo.org/" target="_blank" rel="noopener">Zenodo&lt;/a> using the &lt;a href="https://guides.github.com/activities/citable-code/" target="_blank" rel="noopener">GitHub-Zenodo integration&lt;/a>. Archive any data that hasn&amp;rsquo;t already been archived and that you have permission to archive (typically based on the license allowing &amp;ldquo;redistribution&amp;rdquo; or based on permission from the data collector) on either Zenodo (by archiving it along with the code) or separately on &lt;a href="https://datadryad.org/" target="_blank" rel="noopener">Dryad&lt;/a> (linking to the relevant code through Dryad). Have your code download the already-archived data from the archival source instead of archiving it again. If the journal has open data/code badges, indicate that you want to use them.&lt;/p>
&lt;p>Discussion: Most journals now require the sharing of the code and data used for analyses in a paper. Even in cases where it isn&amp;rsquo;t required this is good for science through supporting reproducibility and building on existing work. Availability of data and code also increases use and citation of papers. There are a variety of locations to archive data. In ecology, general purpose repositories are the norm for most data. Zenodo and Dryad are both non-profits focused on the good of science with good sustainability models. They are also both common locations for archiving so choosing them will avoid any confusion about whether or not the data is properly archived. &amp;ldquo;Archive&amp;rdquo; implies that the data/code will be available in the long-term through guarantees to maintain the infrastructure hosting the data/code and not allowing the person depositing the data/code to remove it. The ability to remove code from GitHub means that GitHub is not considered archival.&lt;/p>
&lt;h2 id="funding-sources">Funding sources&lt;/h2>
&lt;p>Recommendation: Follow journal and funder guidelines for acknowledging funding sources.&lt;/p>
&lt;p>Discussion: Funding agencies require that when their funds are used for research that this funding is acknowledged. Journals have two general approaches to doing this: 1) listing the funding in the Acknowledgements section; 2) having a separate component of the submission process where funding sources are acknowledged. Funding agencies often have specific requirements as well. Check with your advisor &amp;amp; collaborators in the sources that funded the work (you may not always be aware of the different sources) and also make sure to acknowledge any support that was made directly to you (e.g., individual fellowships). Check the funders requirements for for acknowledging their funding or follow an example of a previous paper from the lab with the same funding source.&lt;/p>
&lt;h2 id="opentransparent-review">Open/Transparent Review&lt;/h2>
&lt;p>Recommendation: Participate in open/transparent review.&lt;/p>
&lt;p>It is increasingly common for journals to post reviews and author responses to those reviews online with a paper if it is accepted for publication in the journal. Some journals are also exploring posting reviews even for manuscripts that aren&amp;rsquo;t accepted. The goal is to increase the transparency of the overall scientific process and provide accountability for journals and authors if they have had an important issue pointed out to them that they haven&amp;rsquo;t addressed. Reviewers are typically given the option to remain anonymous. Generally speaking we think open, but anonymous, review history is a positive development and recommend opting in even if it is not required, but this is still a developing area and individual lead authors are welcome to not opt-in (or opt-out if allowed) if they wish after consulting with coauthors.&lt;/p>
&lt;h2 id="submission">Submission&lt;/h2>
&lt;p>Your formatted manuscript and cover letter will get uploaded to the journal&amp;rsquo;s author submission website. There will also be some web forms where they will ask for reviewer/editor suggestions, probably make you enter co-author info (that is already on your cover sheet, but try not to think about the redundancy in the process). Click submit and now you wait for the response, which is often in the 1-2 month, but sometimes closer to 3 months.&lt;/p>
&lt;h2 id="preprints">Preprints&lt;/h2>
&lt;p>Recommendation: Submit preprint to bioRxiv at time of first submission to a journal and update the preprint whenever you submit a revision of the manuscript (either as part of ongoing review or to a new journal after rejection). Check the journals preprint policy (in either the instructions to authors or on &lt;a href="https://v2.sherpa.ac.uk/romeo/" target="_blank" rel="noopener">Sherpa Romeo&lt;/a>) just to make sure they are allowed (though preprints are now allowed basically everywhere) and get sign off from coauthors to post a preprint.&lt;/p>
&lt;p>Discussion: All major journals in ecology that we are aware of now allow posting preprints prior to submission (in part due to &lt;a href="https://jabberwocky.weecology.org/?s=preprint&amp;amp;submit=Search" target="_blank" rel="noopener">Weecology efforts&lt;/a>). They are good for science and professionally beneficial to both early career and established scientists (e.g., &lt;a href="https://doi.org/10.1371/journal.pbio.1001563" target="_blank" rel="noopener">Desjardins-Proulx et al. 2013&lt;/a>, &lt;a href="https://doi.org/10.1073/pnas.1511912112" target="_blank" rel="noopener">Vale 2015&lt;/a>). Benefits to putting up preprints include the possibility that more people will see your work, it will be able to influence the field sooner (as it can takes months or years for papers to be published), it can start to accumulated citations (currently used as a form of academic credit), and you could get valuable feedback to improve the manuscript. Some of this depends on how actively you promote the preprint by sharing on social media, disseminating to other scientists, etc. Journals also identify preprints of interest and could reach out to you about publishing with them.&lt;/p>
&lt;p>In addition, all of our grants include statements that we will submit preprints, so if you decide you do not want to submit a preprint or a coauthor does not want to agree to this please talk to the relevant PI of any grants that have supported you.&lt;/p>
&lt;p>There are a variety of preprint servers, but we almost universally use &lt;a href="http://biorxiv.org/" target="_blank" rel="noopener">bioRxiv&lt;/a>. It has become the standard location in the field and is run by a non-profit organization. Some journals only allow preprints posted to non-profit pre-print servers making it a good choice and it is a good player in ensuring science is available to all.&lt;/p>
&lt;p>We typically submit the initial preprint at the same time as the initial submission of the manuscript to the journal. This will probably take 1-2 hours. To keep the preprint up to date and ensure that it satisfies funder mandates for open archiving it should be updated whenever the manuscript is resubmitted (either as part of a revision or to a new journal).&lt;/p>
&lt;h2 id="sharing-work-on-social-media">Sharing work on social media&lt;/h2>
&lt;p>(info to be included: tone/style; tagging institutions, co-authors, funders; figures and alt-text; links to preprint and code/data)&lt;/p>
&lt;h2 id="dealing-with-reviews">Dealing with reviews&lt;/h2>
&lt;p>When you get your reviews back, the next step is to figure out how to handle them. Typically reviewers will have major and minor criticisms. Your task is to sort through those criticisms and figure out a plan for addressing them. Reviewers are only human and they are not infallible. Their criticisms will fall into one of three categories: useful, neutral, and wrong. You are not required to do everything a reviewers tells you to do. Ethan and Morgan&amp;rsquo;s philosophy (which we got from our advisor) is that if it doesn&amp;rsquo;t harm the research/manuscript you do whatever a reviewer requests (i.e. everything in the neutral/good categories). If you think what the reviewer has suggested is majorly problematic, then we generally advise not doing it. In your response to the reviewers you can explain why you disagree with their suggestion/criticism. You want to choose judiciously when to stand your grand in the review process. Discussion with your co-authors/advisers is warranted.&lt;/p>
&lt;p>When you resubmit your manuscript, you will also submit a detailed response to the reviewers. To construct this, cut and paste the reviews into a new document. Into this document you will write your response to each individual issue the reviewer brings up. Often reviewers will construct their reviews so that each paragraph is focused on a separate distinct issue, if not, you can break their paragraphs accordingly. Keep all of their comments in the document so it is clear you did not edit out any of their concerns. We find that a good starting place for your revision is sitting down with the document and thinking about how you would/should respond to each of these items and start writing in ideas/responses. Then you can use your responses as a template for making the changes in your manuscript.&lt;/p>
&lt;p>Examples of Response to Reviewers:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://ufhcc-bcbsr.github.io/wiki/docs/publishing/point-by-point-response.pdf">Response to reviewers&lt;/a> for &lt;a href="https://doi.org/10.1371/journal.pbio.3000125" target="_blank" rel="noopener">Yenni et al. 2019&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="response-to-editor">Response to editor&lt;/h2>
&lt;p>When you resubmit you will also submit a cover letter to the editor. This is distinct from the detailed response to reviewers. The cover letter will consist of 2 main parts:&lt;/p>
&lt;ul>
&lt;li>a statement similar to the first one in your cover letter stating the name of the manuscript and the authors and that you have revised your manuscript and are resubmitting it.&lt;/li>
&lt;li>a paragraph overviewing the changes you made in response to the reviews - in this paragraph you only really need to discuss what seem like the major concerns - especially if the editor highlighted them in their decision letter.&lt;/li>
&lt;/ul>
&lt;p>Examples of Resubmission letters:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://ufhcc-bcbsr.github.io/wiki/docs/publishing/cover-letter-revision.pdf">Resubmission cover letter&lt;/a> for &lt;a href="https://doi.org/10.1371/journal.pbio.3000125" target="_blank" rel="noopener">Yenni et al. 2019&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="address-for-correspondence">Address for correspondence&lt;/h2>
&lt;p>Use your official university address, which is currently:&lt;/p>
&lt;p>Department of Wildlife Ecology and Conservation&lt;br>
110 Newins-Ziegler Hall&lt;br>
PO Box 110430&lt;br>
Gainesville, FL 32611-0430&lt;/p>
&lt;p>Phone: (352) 846-0643&lt;br>
Fax: (352) 392-6984&lt;/p>
&lt;h2 id="managing-co-authors">Managing co-authors&lt;/h2>
&lt;p>Do not forget your co-authors through this process. You do not have to go through this on your own. They can help you brainstorm how to deal with difficult issues, help run analyses to deal with concerns, help draft language for the response letter or implement revisions to the manuscript.&lt;/p>
&lt;p>Hopefully you have a good relationship with your co-authors and they are actively engaged, however often co-authors are more peripheral to a project (e.g., had a limited role in the analyses/manuscript). It can be easy to forget them as you manage all the moving parts of the review process. Professionally, however, you need to make sure they are OK with the manuscript during all the phases of this process. To do this we recommend following the &lt;a href="https://ufhcc-bcbsr.github.io/wiki/docs/publishing/collaborative-writing/">Collaborative Writing Guide&lt;/a>. Regardless, you must ALWAYS have all co-authors sign off on the final version of the manuscript before submitting.&lt;/p></description></item><item><title>Generative AI</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/generative-ai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/generative-ai/</guid><description>&lt;h2 id="generative-ai-chatbots">Generative AI Chatbots&lt;/h2>
&lt;p>&lt;a href="https://it.ufl.edu/ai/navigator-chat/" target="_blank" rel="noopener">UF Navigator Chat&lt;/a> provides all popular LLMs for free to UF personell. You must be connected to the UF network and authenticate with Duomobile to use this service. We recommend [&amp;hellip;]&lt;/p>
&lt;h2 id="code-copilot-in-vscode">Code Copilot in VSCode&lt;/h2>
&lt;h2 id="uf-policies">UF Policies&lt;/h2>
&lt;p>We encourage you to review &lt;a href="https://privacy.ufl.edu/laws-and-regulations/ai-governance/" target="_blank" rel="noopener">UF AI Governance&lt;/a>. This is critical if you are working with unpublished data, including but not limited to code, data, grant proposals, and manuscripts.&lt;/p>
&lt;h2 id="best-practices-for-prompts">Best Practices for Prompts&lt;/h2></description></item><item><title>Revising a manuscript after review</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/manuscript-revision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/manuscript-revision/</guid><description>&lt;h2 id="journal-decision">Journal Decision&lt;/h2>
&lt;p>After a manuscript has gone through the review process it will be returned with a decision. These generally fall into the following categories:&lt;/p>
&lt;ul>
&lt;li>Accepted - The paper is accepted in it&amp;rsquo;s current form. This never happens on the first round of review.&lt;/li>
&lt;li>Revisions requested - This generally means that the paper has a very good chance of being accepted assuming that you can successfully respond to review comments.&lt;/li>
&lt;li>Rejected, but OK to resubmit - This is what used to be called &amp;ldquo;major revisions&amp;rdquo; and indicates that while there were meaningful concerns that may involve a significant revision the journal is definitely still interested in publishing the paper. Except in exceptional circumstances you should revise and resubmit to the same journal.&lt;/li>
&lt;li>Rejected, without the possibility of resubmission - Very common due to journal scope, perceived importance of the paper, and artificial scarcity imposed by journals, but also due to serious concerns about methods or interpretation. Except in exceptional circumstances the next step is to submit to a different journal.&lt;/li>
&lt;/ul>
&lt;h2 id="actively-involve-co-authors">Actively involve co-authors&lt;/h2>
&lt;ul>
&lt;li>When you receive the reviews and response from the journal, forward it on to your co-authors, even if you are not yet ready to discuss how to handle the comments. If you wait, you&amp;rsquo;ll forget and then you&amp;rsquo;ll be racing a deadline to resubmit without having co-authors engaged.&lt;/li>
&lt;li>It is encouraged to discuss the plan for responding to reviewer critiques with co-authors. Depending on the specifics of the collaboration, different approaches to this may be warranted. You may draft the response to reviewers (see below) and send them out for comments. During the drafting process you may also assign specific comments to specific co-authors if the criticism is pertinent to their role on the manuscript.&lt;/li>
&lt;li>ALWAYS get co-authors to sign off on the final version of the response and manuscript before resubmitting.&lt;/li>
&lt;li>ALWAYS share the good news when a manuscript is accepted. Everyone likes good news. :)&lt;/li>
&lt;/ul>
&lt;h2 id="revising-the-manuscript">Revising the manuscript&lt;/h2>
&lt;p>When you get your reviews back, the next step is to figure out how to handle them. Typically reviewers will have major and minor criticisms. Your task is to sort through those criticisms and figure out a plan for addressing them. Reviewers are only human and they are not infallible. Their criticisms will fall into one of three categories: useful, neutral, and wrong. You are not required to do everything a reviewers tells you to do. Our philosophy is that we make the changes that improve the manuscript as well as any that are neutral. We try to avoid making changes that make the manuscript worse, though there are occasionally strategic tradeoffs to consider.&lt;/p>
&lt;p>The response to review will be composed of three parts:&lt;/p>
&lt;ol>
&lt;li>A point by point response the reviewer comments&lt;/li>
&lt;li>The revisions to the manuscript itself&lt;/li>
&lt;li>A letter to the editor stating general what you&amp;rsquo;ve done&lt;/li>
&lt;/ol>
&lt;p>We recommend proceeding in this order. First developing a draft of the point by point response to reviewer comments and sharing and discussing it with your co-authors to get everyone on the same page about what to do in response to the reviews. Second, following the point by point document, implementing the changes to the manuscript that you have planned. Third, writing the letter to the editor.&lt;/p>
&lt;h3 id="response-to-reviewers">Response to reviewers&lt;/h3>
&lt;p>This is where you explain in detail the changes that you have made in response to the good and neutral comments and also why you disagree with the recommendations that you think would make the manuscript worse. It is also common for reviewers to recommend that you do some new analysis that expands the current manuscript. It is OK to explain that these kinds of expansions of the topic of the paper are out of scope for the current manuscript.&lt;/p>
&lt;ol>
&lt;li>Read through the reviews and then don&amp;rsquo;t work on them for a few days. This will help you respond more positively to the reviews when you revisit them.&lt;/li>
&lt;li>Copy all of the review comments into a single document with a subsection for each reviewer and the editor if the editor provides specific comments. Keep all of their comments in the document so it is clear you did not edit out any of their concerns and reviewers and editors don&amp;rsquo;t have to jump back and forth between review and response documents to evaluate your response.&lt;/li>
&lt;li>After each distinct comment (often reviewers will construct their reviews so that each paragraph is focused on a separate distinct issue, if not, you can break their paragraphs accordingly) write a response describing what you are going to change in the manuscript, analysis, etc. or what is wrong with the suggestion or that the change is out of scope.&lt;/li>
&lt;/ol>
&lt;p>Pay attention for cases where the reviewer comment itself isn&amp;rsquo;t helpful, but indicates a failure to communicate clearly in the current manuscript. In these cases you should revise the manuscript to avoid similar confusion on the part of readers.&lt;/p>
&lt;p>Once you have drafted this, share and discuss it with your co-authors before starting the revise the manuscript itself. This will ensure that everyone agrees on how the paper will be revised (or at least that everyone has had the chance to object to your proposed approach before you do the work). To accomplish this the draft needs to include specific changes that you plan on making, not just general statements that you will make changes in response to a comment.&lt;/p>
&lt;p>Examples of Response to Reviewers:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../point-by-point-response.pdf">Response to reviewers&lt;/a> for &lt;a href="https://doi.org/10.1371/journal.pbio.3000125" target="_blank" rel="noopener">Yenni et al. 2019&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="revisions-to-the-manuscript">Revisions to the manuscript&lt;/h3>
&lt;p>Once everyone agrees on the general form of the response use this response document to guide the revision of the manuscript. This may include adding new analyses, changing existing analyses, modifying figures, and revising the written manuscript.&lt;/p>
&lt;p>Most journals will require you to submit a version of the manuscript with the changes from the previous submission tracked, so make sure you are working in such a way that you can create this at the end.&lt;/p>
&lt;h3 id="response-to-editor">Response to editor&lt;/h3>
&lt;p>When you resubmit you will also submit a cover letter to the editor. This is distinct from the detailed response to reviewers. The cover letter will consist of 2 main parts:&lt;/p>
&lt;ul>
&lt;li>A statement similar to that in the cover letter from the initial submission stating the name of the manuscript and the authors and that you have revised your manuscript and are resubmitting it.&lt;/li>
&lt;li>A paragraph providing an overview of the changes you made in response to the reviews. This paragraph should focus on the major concerns in the reviews, especially any concerns that the editor highlighted in their decision letter. This is your opportunity to briefly explain what positive changes you made in response to the review and to provide a big picture reason for the things you didn&amp;rsquo;t change. This can really set the tone for how the response to reviewers is read by the editor.&lt;/li>
&lt;/ul>
&lt;p>Examples of Resubmission letters:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../cover-letter-revision.pdf">Resubmission cover letter&lt;/a> for &lt;a href="https://doi.org/10.1371/journal.pbio.3000125" target="_blank" rel="noopener">Yenni et al. 2019&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>scRNA-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/analysis-pipelines/scrna-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/analysis-pipelines/scrna-seq/</guid><description>&lt;h2 id="submit-a-support-request-form">Submit a Support Request Form&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://cancer.ufl.edu/research/shared-resources/biostatistics-computational-biology-shared-resource/biostatistics-shared-resource-support-request-form/" target="_blank" rel="noopener">Online Support Requet Form&lt;/a>:&lt;/li>
&lt;/ul>
&lt;p>Fill out the support request form as best you can. The important thing is to get connected with us, we&amp;rsquo;ll learn much more about your project later.&lt;/p>
&lt;h2 id="schedule-a-meeting-with-us">Schedule a Meeting With Us&lt;/h2>
&lt;p>Once we receive your support request, we&amp;rsquo;ll send you an email to schedule a meeting. Initial meetings are usually about an hour long, during which we want to understand your project and the expectations of a future collaboration.&lt;/p>
&lt;h2 id="how-does-the-funding-work">How Does the Funding Work?&lt;/h2>
&lt;p>The BCB-SR is a group of bioinformatics scientists whose salaries are primarily funded by effort on the grants on which they collaborate. Because of this, the ideal scenario is that you identify the need for bioinformatics collaboration &lt;strong>before&lt;/strong> your grant is submitted so that there will be an appropriate allocation of funding. If this is not the case, we will estimate our effort given the project&amp;rsquo;s needs and determine whether the project has sufficient funding available to support bioinformatics collaboration.&lt;/p></description></item><item><title>scRNA-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/differential-analysis/scrna-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/differential-analysis/scrna-seq/</guid><description>&lt;h2 id="submit-a-support-request-form">Submit a Support Request Form&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://cancer.ufl.edu/research/shared-resources/biostatistics-computational-biology-shared-resource/biostatistics-shared-resource-support-request-form/" target="_blank" rel="noopener">Online Support Requet Form&lt;/a>:&lt;/li>
&lt;/ul>
&lt;p>Fill out the support request form as best you can. The important thing is to get connected with us, we&amp;rsquo;ll learn much more about your project later.&lt;/p>
&lt;h2 id="schedule-a-meeting-with-us">Schedule a Meeting With Us&lt;/h2>
&lt;p>Once we receive your support request, we&amp;rsquo;ll send you an email to schedule a meeting. Initial meetings are usually about an hour long, during which we want to understand your project and the expectations of a future collaboration.&lt;/p>
&lt;h2 id="how-does-the-funding-work">How Does the Funding Work?&lt;/h2>
&lt;p>The BCB-SR is a group of bioinformatics scientists whose salaries are primarily funded by effort on the grants on which they collaborate. Because of this, the ideal scenario is that you identify the need for bioinformatics collaboration &lt;strong>before&lt;/strong> your grant is submitted so that there will be an appropriate allocation of funding. If this is not the case, we will estimate our effort given the project&amp;rsquo;s needs and determine whether the project has sufficient funding available to support bioinformatics collaboration.&lt;/p></description></item><item><title>scRNA-seq</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/experimental-design/scrna-seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/experimental-design/scrna-seq/</guid><description>&lt;h2 id="submit-a-support-request-form">Submit a Support Request Form&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://cancer.ufl.edu/research/shared-resources/biostatistics-computational-biology-shared-resource/biostatistics-shared-resource-support-request-form/" target="_blank" rel="noopener">Online Support Requet Form&lt;/a>:&lt;/li>
&lt;/ul>
&lt;p>Fill out the support request form as best you can. The important thing is to get connected with us, we&amp;rsquo;ll learn much more about your project later.&lt;/p>
&lt;h2 id="schedule-a-meeting-with-us">Schedule a Meeting With Us&lt;/h2>
&lt;p>Once we receive your support request, we&amp;rsquo;ll send you an email to schedule a meeting. Initial meetings are usually about an hour long, during which we want to understand your project and the expectations of a future collaboration.&lt;/p>
&lt;h2 id="how-does-the-funding-work">How Does the Funding Work?&lt;/h2>
&lt;p>The BCB-SR is a group of bioinformatics scientists whose salaries are primarily funded by effort on the grants on which they collaborate. Because of this, the ideal scenario is that you identify the need for bioinformatics collaboration &lt;strong>before&lt;/strong> your grant is submitted so that there will be an appropriate allocation of funding. If this is not the case, we will estimate our effort given the project&amp;rsquo;s needs and determine whether the project has sufficient funding available to support bioinformatics collaboration.&lt;/p></description></item><item><title>Authorship</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/authorship/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/authorship/</guid><description>&lt;p>[In Progress]&lt;/p>
&lt;p>Trying to figure out who should be an author on a project - especially one developed in a group context - can be complicated. How much involvement warrants authorship credit is not clearly defined or nor is someone&amp;rsquo;s involvement easily quantified. The Weecology Philosophy is that it is better to be inclusive than exclusive. This is because it is easy to discount people&amp;rsquo;s contributions based on internalized biases. Some of these biases are things we have from our society. Some of these biases are because we have pre-conceived notions about how difficult/important/meaningful certain contributions are. If someone has done something for a project&lt;/p></description></item><item><title>Authorship</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/publishing/authorship/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/publishing/authorship/</guid><description>&lt;p>[In Progress]&lt;/p>
&lt;p>Trying to figure out who should be an author on a project - especially one developed in a group context - can be complicated. How much involvement warrants authorship credit is not clearly defined or nor is someone&amp;rsquo;s involvement easily quantified. The Weecology Philosophy is that it is better to be inclusive than exclusive. This is because it is easy to discount people&amp;rsquo;s contributions based on internalized biases. Some of these biases are things we have from our society. Some of these biases are because we have pre-conceived notions about how difficult/important/meaningful certain contributions are. If someone has done something for a project&lt;/p></description></item><item><title>Bioinformatics Blogs</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/blog/bioinfo-blogs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/blog/bioinfo-blogs/</guid><description>&lt;p>&lt;a href="https://divingintogeneticsandgenomics.com/" target="_blank" rel="noopener">Ming &amp;ldquo;Tommy&amp;rdquo; Tang&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://davetang.org/muse/" target="_blank" rel="noopener">Dave Tang&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://ivory.idyll.org/blog/tag/bioinformatics.html" target="_blank" rel="noopener">C. Titus Brown&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://jef.works/blog/2024/11/04/using-ai-to-prevent-manels/" target="_blank" rel="noopener">Jean Fan&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://douglasyao.github.io/blog/" target="_blank" rel="noopener">Douglas Yao&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://ekernf01.github.io/blog/" target="_blank" rel="noopener">Eric Kernfeld&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://joynyaanga.com/project_landing/" target="_blank" rel="noopener">Joy Nyaanga&lt;/a>&lt;/p></description></item><item><title>Lab Style Guide for Code</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/lab-code-style-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/lab-code-style-guide/</guid><description>&lt;h2 id="guiding-principles">Guiding Principles&lt;/h2>
&lt;p>This document provides a guide to code structure and formatting across languages used within the Weecology projects. Links to language-specific guides are provided below.&lt;/p>
&lt;p>Generally, this guide follows the principles outlined
. In particular:&lt;/p>
&lt;ol>
&lt;li>Write and style your code for human readers&lt;/li>
&lt;li>Minimize the number of facts a reader is expected to hold at one time&lt;/li>
&lt;li>Use consistent, distinct, and meaningful names&lt;/li>
&lt;li>Employ consistent style and formatting&lt;/li>
&lt;/ol>
&lt;h2 id="structure">Structure&lt;/h2>
&lt;h3 id="modularize">Modularize&lt;/h3>
&lt;ul>
&lt;li>Break code into chunks corresponding to contained tasks&lt;/li>
&lt;li>Whenever possible write code into functions, even if the function isn&amp;rsquo;t called repeatedly&lt;/li>
&lt;/ul>
&lt;h3 id="loops">Loops&lt;/h3>
&lt;ul>
&lt;li>Loop should be used with repeated tasks unless you actually need the indices&lt;/li>
&lt;li>If the language allows, use vectorized functions in place of loops to speed computation and reduce code volume&lt;/li>
&lt;/ul>
&lt;h2 id="style">Style&lt;/h2>
&lt;h3 id="naming">Naming&lt;/h3>
&lt;ul>
&lt;li>Be concise and meaningful, but conveying meaning is more important than brevity
&lt;ul>
&lt;li>Document abbreviations if they are not common or immediately intuitive&lt;/li>
&lt;li>Functions are verbs, variables are nouns&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Use snake_case for variables and functions (e.g., &lt;code>portal_data&lt;/code>)
&lt;ul>
&lt;li>Exceptions:
&lt;ul>
&lt;li>established prefixes (e.g., &lt;code>n&lt;/code> in &lt;code>nobs&lt;/code> to indicate the number of observations)&lt;/li>
&lt;li>established suffixes (e.g., &lt;code>i&lt;/code> in &lt;code>obsi&lt;/code> to indicate the specific observation in a for loop)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Use UpperCamelCase for class names for object oriented programming (primarily in Python)&lt;/li>
&lt;li>Do not use &lt;code>.&lt;/code> in names (
)&lt;/li>
&lt;li>Do not use single-letter names
&lt;ul>
&lt;li>Exceptions:
&lt;ul>
&lt;li>representing a term in an equation (e.g., &lt;code>y&lt;/code> in &lt;code>y = m * x + b&lt;/code>)&lt;/li>
&lt;li>using an established name in a language (e.g., &lt;code>n&lt;/code> references the number of draws from a random variable in R)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Constants, and only constants, should be in all caps&lt;/li>
&lt;/ul>
&lt;h3 id="white-space">White space&lt;/h3>
&lt;ul>
&lt;li>spaces after commas&lt;/li>
&lt;li>spaces around operators (unless inside the argument definitions in Python)&lt;/li>
&lt;li>no spaces around parentheses&lt;/li>
&lt;/ul>
&lt;h3 id="line-length">Line length&lt;/h3>
&lt;ul>
&lt;li>Lines &amp;lt;= 80 characters
&lt;ul>
&lt;li>But a few extra characters can be better than confusing contortions to make length&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Parentheses/brackets/braces with breaks after commas are typically better than line break characters (but not always)&lt;/li>
&lt;/ul>
&lt;h3 id="indentation">Indentation&lt;/h3>
&lt;ul>
&lt;li>Always indent to indicate that code is inside a function, loop, etc. (Python makes you do this. Thanks Python!)&lt;/li>
&lt;li>Use spaces, not tabs (but it&amp;rsquo;s fine for you IDE to turn the tab key into spaces)&lt;/li>
&lt;li>Follow language convention for number of spaces
&lt;ul>
&lt;li>R: 2&lt;/li>
&lt;li>Python: 4&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>When breaking lines with parentheses (mostly in function calls/definitions) align with the leading character after the opening parenthesis
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">(stuff, things,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> more_things)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;h3 id="references">References&lt;/h3>
&lt;ul>
&lt;li>Magic numbers (numeric references to elements, columns, rows, etc.) should be avoided.&lt;/li>
&lt;li>References should be made by name or-in the case of loops-position.&lt;/li>
&lt;/ul>
&lt;h2 id="documentation">Documentation&lt;/h2>
&lt;h3 id="in-line-commenting">In-line commenting&lt;/h3>
&lt;ul>
&lt;li>
&lt;/li>
&lt;/ul>
&lt;h3 id="external-documentation">External documentation&lt;/h3>
&lt;ul>
&lt;li>Use standard documentation comment styles
&lt;ul>
&lt;li>R:
&lt;/li>
&lt;li>Python:
&lt;/li>
&lt;li>These can create formatted documentation, but they are useful visual indicators even if you don&amp;rsquo;t do this&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="language-specific-style-guides">Language specific style guides&lt;/h2>
&lt;ul>
&lt;li>Follow official language style guides (within reason). This helps make your code broadly readable and makes external contributions more likely.
&lt;ul>
&lt;li>Python:
&lt;/li>
&lt;li>Julia:
&lt;/li>
&lt;li>R:
. This isn&amp;rsquo;t official, or broadly agreed on, but it serves as the base (or at least justification) for a lot of what we do&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Collaborative Writing Process</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/collaborative-writing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/collaborative-writing/</guid><description>&lt;p>Almost all scientific papers now involve multiple authors.
Therefore having processes to engage all authors and solicit feedback in ways that improve the project, support the collaborators, and minimize wasted time are important.
Instead of working on it a manuscript on our own until you have a full draft, actively engaging collaborators early and often can lead better papers with less pain, by ensureing that everyone is on board with
the underlying questions, methodological decisions, visualization and other aspects of the project.&lt;/p>
&lt;p>&lt;em>Collaborative writing guidelines&lt;/em>&lt;/p>
&lt;h2 id="step-1">Step 1&lt;/h2>
&lt;p>Check with your adviser and collaborators to see if they agree that a set of analyses has reached that poitn that it should be written up as a paper.&lt;/p>
&lt;h2 id="step-2">Step 2&lt;/h2>
&lt;p>Define the knowledge gap that the paper will fill and the 2 or 3 key points that you want to communicate with the readers. This step helps engage collaborators in identifying why and where
your work is providing a better understanding/improvement of existing knowledge gaps. Write up the knowledge gap(s) and key points as bullet points and send them to your collaborators/adviser for feedback.
Lead a discussion to reach consensus on these ideas.&lt;/p>
&lt;h2 id="step-3">Step 3&lt;/h2>
&lt;p>Write a rough draft of a title and produce drafts of the key figures you envision including in the manuscript.
Send these to your collaborators/adviser and lead a discussion to reach consensus on the title and figures to be included.
This helps further define the paper and helps address any issues with analyses including identifying additional analyses that need to be conducted.&lt;/p>
&lt;h2 id="step-4">Step 4&lt;/h2>
&lt;p>Draft the Methods section and send it to your collaborators/adviser for feedback.
While everyone often thinks they know what has been done methodologically in a collaboration, writing it down can reveal differences in understanding about what exactly was done.
Writing and discussing this section first helps identify any analysis changes that need to be made &lt;em>before&lt;/em> a lot of time is invested in understanding the results (which could change if the analysis changes).&lt;/p>
&lt;h2 id="step-5">Step 5&lt;/h2>
&lt;p>Write an outline. Once everybody is on board, write an outline of the paper with each section of the paper including roughly one bullet points per paragraph.&lt;/p>
&lt;ul>
&lt;li>Send these to your collaborators/adviser and lead a discussion to reach consensus on the outline.
At this point everyone will have had several opportunities to engage and come to consensus about what should be included in the paper.&lt;/li>
&lt;/ul>
&lt;h3 id="step-5b-optional">Step 5b (optional)&lt;/h3>
&lt;p>At this point you can optionally expand each paragraph to sentence level bullet points, e.g., 3-4 words per bulletpoint.
This can be very helpful and save lots of time for folks still learning to write scientific papers, because the next step then involves only expanding each bullet point into a sentence.&lt;/p>
&lt;h2 id="step-6">Step 6&lt;/h2>
&lt;p>Write a &lt;em>rough&lt;/em> draft of the manuscript and send it to collaboratators for &amp;ldquo;big picture&amp;rdquo; feedback.
You can either do this a section at a time or draft the full manuscript and then send it to everyone.
Collaborators should be encouraged to read the draft and provide big picture suggestions identifying things that are missing, any additional work that needs to be added, and areas that need major improvement in communication.
They should not provide detailed editorial feedback at this stage.
Ideally this phase should be focused on the writing, but it also serves as a last chance to identify important analyses that are missing or need to be changed.&lt;/p>
&lt;h2 id="step-7">Step 7&lt;/h2>
&lt;p>Incorporate collaborator feedback and revise the paper into a polished draft.
Send the manuscript to co-authors for feedback communicating what you expect from collaborators at this point is mainly editorial work and signoffs on the manuscript.
The goal of the previous steps is to ensure that no changes to the methods, figures, and questions needs to be made at this stage.
However, if something critical is identified at this stage that could influence the conclusions of the paper, then it is still important to address it.&lt;/p></description></item><item><title>Computer Setup - Mac</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/computer-setup-mac/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/computer-setup-mac/</guid><description>&lt;h2 id="terminal">Terminal&lt;/h2>
&lt;p>This is an application on Macs. It is opened by double-clicking on the icon, which should bring up a small window with a black background. The Terminal is used to move around in the file directory, rearrange files, and use git.&lt;/p>
&lt;h2 id="python">Python&lt;/h2>
&lt;p>&lt;em>IDE&lt;/em>&lt;/p>
&lt;p>&lt;em>Packages&lt;/em>&lt;/p>
&lt;p>&lt;em>Projects&lt;/em>&lt;/p>
&lt;p>&lt;em>Links to get started coding in Python&lt;/em>&lt;/p>
&lt;p>&lt;em>Using git/GitHub&lt;/em>&lt;/p>
&lt;h2 id="r">R&lt;/h2>
&lt;p>&lt;em>Installing R&lt;/em>&lt;/p>
&lt;p>&lt;em>Installing RStudio&lt;/em>&lt;/p>
&lt;p>&lt;em>Packages&lt;/em>&lt;/p>
&lt;p>&lt;em>Projects&lt;/em>&lt;/p>
&lt;p>&lt;em>Using git/GitHub&lt;/em>&lt;/p>
&lt;h2 id="dependencies">Dependencies&lt;/h2>
&lt;h2 id="gitgithub">Git/GitHub&lt;/h2>
&lt;p>Resources: &lt;a href="http://swcarpentry.github.io/git-novice/" target="_blank" rel="noopener">Software Carpentry&lt;/a>, &lt;a href="https://happygitwithr.com/" target="_blank" rel="noopener">Happy Git and GiHub for the userR&lt;/a>, &lt;a href="http://rogerdudler.github.io/git-guide/" target="_blank" rel="noopener">Roger Dudler cheatsheet&lt;/a>&lt;/p>
&lt;h3 id="installing-git">Installing git&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Open up the Terminal, type in &amp;ldquo;git&amp;rdquo; and press enter.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>This should cause a pop-up window to appear. It will have several options; click on &amp;ldquo;Install&amp;rdquo; (not &amp;ldquo;Get Xcode&amp;rdquo;, see &amp;ldquo;Installing Xcode&amp;rdquo; for that).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &amp;ldquo;Agree&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When the install is finished, click &amp;ldquo;Done&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To make sure this worked, type in &amp;ldquo;git&amp;rdquo; in the Terminal and press enter. Some information will come up, including a list of common commands.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="configuring-git">Configuring Git&lt;/h3>
&lt;p>There&amp;rsquo;s some basic info on Git setup from &lt;a href="https://swcarpentry.github.io/git-novice/#installing-git" target="_blank" rel="noopener">software carpentry&lt;/a>. If you are also setting up a GitHub account, be sure to use the same email address, so that when you use Git on your computer and &lt;em>push&lt;/em> the changes to GitHub, it identifies you correctly.&lt;/p>
&lt;p>On a mac, browsing folders in finder also tends to generate &lt;code>.DS_Store&lt;/code> files. You generally don&amp;rsquo;t want to include those in your repositories, so here are &lt;a href="https://www.jeffgeerling.com/blogs/jeff-geerling/stop-letting-dsstore-slow-you" target="_blank" rel="noopener">some instructions&lt;/a> to ignore such files globally.&lt;/p>
&lt;h3 id="installing-xcode">Installing Xcode&lt;/h3>
&lt;h3 id="github">GitHub&lt;/h3>
&lt;p>Create a GitHub account by going to &lt;a href="https://github.com/" target="_blank" rel="noopener">https://github.com/&lt;/a>&lt;/p>
&lt;p>This is a service that allows you to store your code, project management materials, etc., online. It allows for other people to look at your work (if the repo is public). It also saves all the versions of your code as you change it, which is referred to as version control. This eliminates the need for creating multiple copies of code as you change it (e.g., folder with files called &amp;ldquo;data_analysis&amp;rdquo;, &amp;ldquo;data_analysis_3&amp;rdquo;, &amp;ldquo;data_analysis_45&amp;rdquo;, &amp;ldquo;data_analysis_final&amp;rdquo;, and &amp;ldquo;data_analysis_final_really&amp;rdquo;) and you can use the online GitHub interface to easily look back at previous versions of your code and see what was changed.&lt;/p>
&lt;h3 id="repositories">Repositories&lt;/h3>
&lt;p>A repository is where you put all of the materials related to single project. One repository per project.&lt;/p>
&lt;p>Creating a GitHub repository:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Open up a browser, go to the GitHub website, and sign into your GitHub account. Navigate to your profile page.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click on the &amp;ldquo;Repositories&amp;rdquo; tab in the top middle of the page.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the upper right hand corner, click on the green &amp;ldquo;New&amp;rdquo; button.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In this page, name the repo, preferably something short and succinct that uniquely describes the project. Also, there should be no spaces in repository names. If the name consists of multiple words, separate them by an underscore, dash, or camel case, e.g., mammal_community_dynamics, mammal-community-dynamics, MammalCommunityDynamics.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Select if the repo will be public or private. Keep in mind that you have a limited number of private repos for free, so most of your repos will likely be public. Having your research publicly available also makes for better science.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check &amp;ldquo;Initialize this repository with a README&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Leave both &amp;ldquo;Add .gitignore&amp;rdquo; and &amp;ldquo;Add a license&amp;rdquo; as &amp;ldquo;None&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click green &amp;ldquo;Create repository&amp;rdquo; button. You&amp;rsquo;ve created your new repository, congrats!&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Cloning GitHub repository:&lt;/p>
&lt;p>The repository that was created above is the remote repository. This can be accessed from any computer using the browser. You also need to create a copy of the remote repository called the local repository. This copy of the repo can only be accessed from the computer it is created on. You will do work (e.g., changing code) on the local repo.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>In the browser, navigate to the main page of the repository you want to clone.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the right-hand column near the bottom, there is a bar containing a URL. There are two possible options for this URL, either HTTPS or SSH. You can switch between these two by clicking on the relevant blue hyperlinked acronym below the URL.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click on the HTTPS blue hyperlinked word. Either HTTPS or SSH can be used, but it is easier to start with HTTPS. The difference between these is how they link the local repo to the remote, but that difference is not important right now.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Copy the HTTPS URL.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Open up the Terminal and navigate to the location on your computer where you want the local repo to be located. You can navigate around using the commands &amp;ldquo;ls&amp;rdquo; (this displays all the folders and files in the current directory) and &amp;ldquo;cd&amp;rdquo;. This latter command changes the directory, so you will type in the path for the directory you want to go to. For example, if I want to put the repo in the folder Projects, which is within the folder Documents, I would type the following into the Terminal: &amp;ldquo;cd Documents/Projects&amp;rdquo;. Then hit enter.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once you&amp;rsquo;re in the directory where you want the repo to be located, type in the command &amp;ldquo;git clone&amp;rdquo;, a space, and then paste in the HTTPS URL.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hit enter. This should create a new folder in the chosen directory that has the same name as the remote repo. This is your local repository.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Adding files and commits to local repository:&lt;/p>
&lt;p>An important aspect of git/GitHub is version control. As you change scripts, you can use git to save all the different versions of scripts and then use GitHub later on to easily look at each of these versions. Then, if you mess up your code or don&amp;rsquo;t like the direction it&amp;rsquo;s heading in, you can access and use a previous version of your script very easily.&lt;/p>
&lt;p>The way that you save these versions using git is by doing something called a commit. Each commit represents a different version of a script. Because you choose when to do a commit, you get to choose how different all of the versions of the script are. You should definitely make a commit for every major change in the code that you make, but you can never commit too often. When in doubt, commit.&lt;/p>
&lt;p>Another important, and confusing, step in this process is adding the script. Before you can commit the newest version of a script, you have to add that script to the stage. This means that if you&amp;rsquo;ve changed several of the scripts within one local repository, you can add all of these scripts to the stage and commit them together, or you can add one script to the stage and commit it at a time.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create a new script (in Python, R, or whatever language) and save the script in the folder for the local repository you&amp;rsquo;ve just created. Similar to the names of repositories, there should be no spaces in script names. See the fourth bullet point in the &amp;ldquo;Creating a GitHub Repository&amp;rdquo; section for naming conventions.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you are not already there, open up the Terminal and navigate to the local repository folder using the &amp;ldquo;cd&amp;rdquo; command.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To add the script, type in &amp;ldquo;git add &amp;quot; and then the name of the script, and then hit enter. (There should be a space between the add command and the script name). The script is now on the stage. Optional: You can repeat this multiple times in a row with different script names if you&amp;rsquo;re adding multiple scripts to the stage at the same time.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To make sure that the script has been added, type in &amp;ldquo;git status&amp;rdquo; and hit Enter. This will bring up information about the repo that you are currently looking at. If you&amp;rsquo;ve correctly added the script, under the &amp;ldquo;Changes to be committed:&amp;rdquo; header, there should be an indented bit of text that will be formatted as &amp;ldquo;modified: &amp;quot; and the name of the script you&amp;rsquo;ve added.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Now you want to commit this version of the script. In the Terminal, type in &amp;ldquo;git commit -m &amp;ldquo;&lt;em>message&lt;/em>&amp;rdquo; and hit enter. The &lt;em>message&lt;/em> is where you will insert a succinct, informative description of what changed between the last version and this newest version of the script. Writing good commit messages is a bit of an art, but there is some information [here] (&lt;a href="http://chris.beams.io/posts/git-commit/" target="_blank" rel="noopener">http://chris.beams.io/posts/git-commit/&lt;/a>) on good commit messages.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You can use git status again to check that the commit worked. Type in &amp;ldquo;git status&amp;rdquo; and hit Enter. Now the entire &amp;ldquo;Changes to be committed:&amp;rdquo; section should be gone, because there should no longer be any changes that haven&amp;rsquo;t been committed in this repo.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You can look at this commit, and all previous commits, by typing in &amp;ldquo;git log&amp;rdquo; and hitting Enter. This will bring up a list of the commits, with the most recent commit at the top. The information about each commit includes the author, date, and message of the commit. At the top of each commit, there is a long string of letters and numbers. This is the hash, or unique identifier, for each commit.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You will do this add and commit workflow (steps 2-7) each time you make a substantial change to this script, or when you want to add another script.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Pushing and pulling:&lt;/p>
&lt;p>TODO: add summary here (about getting changes up to GitHub repository)&lt;/p></description></item><item><title>Containers</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/containers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/containers/</guid><description>&lt;p>Containers are a tool for running code in self-contained environments.&lt;/p>
&lt;h2 id="debugging-r-devel-using-docker">Debugging r-devel using Docker&lt;/h2>
&lt;p>First &lt;a href="https://www.docker.com/get-started" target="_blank" rel="noopener">install docker&lt;/a> for your operating system.&lt;/p>
&lt;p>Then get an r-devel container:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">sudo docker pull rocker/drd
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The run docker interactively while mounting your working directory:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">sudo docker run -v WORKING/DIRECTORY:/mnt/WORKDIR -it rocker/drd:latest
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Replace &lt;code>WORKING/DIRECTORY&lt;/code> with the path to the directory whose files you want to access and &lt;code>WORKDIR&lt;/code> with whatever you want it to be named inside the &lt;code>/mnt/&lt;/code> directory in the container.&lt;/p>
&lt;p>This will open an interactive R console running r-devel.&lt;/p>
&lt;h2 id="using-containers-in-vs-code">Using containers in VS Code&lt;/h2>
&lt;p>If you use VS Code as your IDE you can develop inside a container.
To setup this up follow &lt;a href="https://code.visualstudio.com/docs/devcontainers/containers" target="_blank" rel="noopener">the official instructions&lt;/a>, which can be summarized as:&lt;/p>
&lt;ol>
&lt;li>Install docker for your OS with non-sudo access (on Linux add your user to the &lt;code>docker&lt;/code> group and logout)&lt;/li>
&lt;li>Install the Dev Containers extension&lt;/li>
&lt;li>Create a &lt;code>.devcontainers/devcontainer.json&lt;/code> file to your project directory indicating which container to use, e.g,.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="line">&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;r-devel&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;image&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;rocker/drd:latest&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This can be a little tricky to setup (don&amp;rsquo;t be afraid to ask for help), but when you open the project in VS Code you&amp;rsquo;ll be automatically working in the designated container.&lt;/p></description></item><item><title>File Compression Notes</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/file-compression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/file-compression/</guid><description>&lt;h2 id="efficient-large-volume-decompression">Efficient large volume (de)compression&lt;/h2>
&lt;p>When archiving large volumes of data using parallel and highly efficient algorithms can be useful.
We most commonly do this when archiving old projects on the HPC.&lt;/p>
&lt;p>On Linux (and our HPC) one of the easy ways to do this is with tar with zstd compression.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">tar --use-compress-program&lt;span class="o">=&lt;/span>zstd -cvf my_archive.tar.zst /path/to/archive
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you need to pass arguments to zstd they can be included in quotes, &lt;code>'zstd -v'&lt;/code>.&lt;/p>
&lt;p>To uncompress these archives:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">tar --use-compress-program&lt;span class="o">=&lt;/span>unzstd -xvf my_archive.tar.zst
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ignoring-failed-reads-using-tar">Ignoring failed reads using tar&lt;/h2>
&lt;p>When archiving files with tar the archive will fail if any file cannot be read by the account doing the archiving.
This is a common occurrence we archiving on the HPC and the files are often (but not always) hidden files that don&amp;rsquo;t need to be archived (but definitely check to make sure).
You can ignored these failed reads using the &lt;code>--ignore-failed-read&lt;/code> flag.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">tar --ignore-failed-read -cvf my_archive.tar.zst /path/to/archive
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="fixing-a-corrupted-zip-file">Fixing a corrupted zip file&lt;/h2>
&lt;h3 id="using-zip">Using zip&lt;/h3>
&lt;p>If you try to open a zip file and it won&amp;rsquo;t unzip you can often fix it by rezipping the file (&lt;a href="https://superuser.com/questions/23290/terminal-tool-linux-for-repair-corrupted-zip-files" target="_blank" rel="noopener">source&lt;/a>).&lt;/p>
&lt;p>First, try:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">zip -F corrupted.zip --out fixed.zip
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If that doesn&amp;rsquo;t work try:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">zip -FF corrupted.zip --out fixed.zip
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you receive an error message like:&lt;/p>
&lt;blockquote>
&lt;p>zip error: Entry too big to split, read, or write (Poor compression resulted in unexpectedly large entry - try -fz)&lt;/p>&lt;/blockquote>
&lt;p>then:&lt;/p>
&lt;ol>
&lt;li>Make sure you have at least version 3.0 of &lt;code>zip&lt;/code>&lt;/li>
&lt;li>Try adding &lt;code>-fz&lt;/code> to the command&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">zip -FF -fz corrupted.zip --out fixed.zip
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-p7zip">Using p7zip&lt;/h3>
&lt;p>If none of this works try &lt;a href="https://7-zip.org/" target="_blank" rel="noopener">p7zip&lt;/a>, which can be installed using conda.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">conda create -n p7zip &lt;span class="nv">python&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda activate p7zip
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda install -c bioconda p7zip
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This version is pretty out of date, but much less out of date than the one in the HiperGator module system.
The one in the HiperGator module is too old to solve the problems we&amp;rsquo;ve seen.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">7za x corrupted.zip
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this will decompress into the current working directory not into &lt;code>./corrupted/&lt;/code>&lt;/p>
&lt;h2 id="increasing-compression">Increasing compression&lt;/h2>
&lt;p>There is a tradeoff between how long it takes to compress something and how much smaller gets.
When using &lt;code>zip&lt;/code> this is controlled by a numeric argument ranging from 1 (faster) to 9 (smaller).
So, if you&amp;rsquo;re archiving large objects try using &lt;code>zip -9&lt;/code>.&lt;/p>
&lt;p>If you have a bunch of already zipped files you can recompress them using the following bash loop:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> f in *.zip
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">do&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> mkdir &lt;span class="si">${&lt;/span>&lt;span class="nv">f&lt;/span>&lt;span class="p">%.*&lt;/span>&lt;span class="si">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> unzip -d &lt;span class="si">${&lt;/span>&lt;span class="nv">f&lt;/span>&lt;span class="p">%.zip&lt;/span>&lt;span class="si">}&lt;/span> &lt;span class="nv">$f&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> rm &lt;span class="nv">$f&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> rm &lt;span class="si">${&lt;/span>&lt;span class="nv">f&lt;/span>&lt;span class="p">%.*&lt;/span>&lt;span class="si">}&lt;/span>/&lt;span class="si">${&lt;/span>&lt;span class="nv">f&lt;/span>&lt;span class="p">%.*&lt;/span>&lt;span class="si">}&lt;/span>.csv
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> zip -r -9 &lt;span class="nv">$f&lt;/span> &lt;span class="si">${&lt;/span>&lt;span class="nv">f&lt;/span>&lt;span class="p">%.zip&lt;/span>&lt;span class="si">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">done&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Collaborating Using Git &amp; GitHub</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/git-collaboration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/git-collaboration/</guid><description>&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>This is intended to be a default set of procedures for Weecologists to collaborate together using a Git/GitHub repository. For projects that are primarily being worked on by one person, this is probably unnecessary, but you may want to follow this anyway, so as to ingrain the workflow practices.&lt;/p>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>&lt;em>If you haven&amp;rsquo;t done so already, please check out the onboarding
.&lt;/em>&lt;/p>
&lt;p>In this guide, we presume that there is a single repo on GitHub and multiple users, who work on clones of that repo (on their local machines), and interface through GitHub.&lt;/p>
&lt;h2 id="branching">Branching&lt;/h2>
&lt;p>One way of thinking about git branches is that each branch represents a &amp;ldquo;lineage&amp;rdquo; of commits in a repo. By default, git repos have a &lt;code>master&lt;/code> branch, and adding commits to a new repo will create iterative versions of the project, all considered to be part of the &lt;code>master&lt;/code> branch.&lt;/p>
&lt;p>You can see the branches in your project using &lt;code>git branch&lt;/code> from the command line while in the folder with a git repo. This will list the branches in the repo:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git branch
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> biomass-function
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> hao-data-vignette
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">* master
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, &lt;code>master&lt;/code> is marked with an asterisk (and possibly a different color) to indicate that it is the &amp;ldquo;active&amp;rdquo; branch. What this means is that new commits added to the repo will be derived from the end of the master branch and included as part of that branch.&lt;/p>
&lt;h3 id="making-new-branches">Making New Branches&lt;/h3>
&lt;p>We can create new branches by specifying a new branch name when using the &lt;code>git branch&lt;/code> command. This allows us to start a new &amp;ldquo;lineage&amp;rdquo; of commits from the current state of the repo.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git branch hao-test-branch
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When we look at the branches, we now see:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git branch
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> biomass-function
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> hao-data-vignette
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> hao-test-branch
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">* master
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice that the active branch is still &amp;ldquo;master&amp;rdquo;.&lt;/p>
&lt;h3 id="switching-branches">Switching Branches&lt;/h3>
&lt;p>To change the active branch, we use the &lt;code>git checkout&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git checkout hao-test-branch
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Switched to branch &amp;#39;hao-test-branch&amp;#39;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is what it looks like when we run &lt;code>git branch&lt;/code> afterword:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git branch
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> biomass-function
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> hao-data-vignette
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">* hao-test-branch
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> master
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="pushing-to-github">Pushing to GitHub&lt;/h3>
&lt;p>After we have created a branch on our local clone of the repo, and made some commits, we might want to push those commits to GitHub. The first time we do so, however, we encounter an error:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git push
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">fatal: The current branch hao-test-branch has no upstream branch.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">To push the current branch and set the remote as upstream, use
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> git push --set-upstream origin hao-test-branch
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The reason for this error is that the repo on GitHub does not have the branch &lt;code>hao-test-branch&lt;/code>, and commits have to be assigned to a branch. The suggested command does several things at once:&lt;/p>
&lt;ol>
&lt;li>create a branch called &lt;code>hao-test-branch&lt;/code> on the GitHub repo (which has the remote name &lt;code>origin&lt;/code>)&lt;/li>
&lt;li>establish a link between the local branch called &lt;code>hao-test-branch&lt;/code> and the GitHub branch called &lt;code>hao-test-branch&lt;/code>&lt;/li>
&lt;li>push the local commits on &lt;code>hao-test-branch&lt;/code> to GitHub.&lt;/li>
&lt;/ol>
&lt;h3 id="pulling-from-github">Pulling from GitHub&lt;/h3>
&lt;p>Suppose someone starts making an update and has pushed it to GitHub and wants your help before merging it into the master branch. How do you download that new branch?&lt;/p>
&lt;p>First, make sure we get all the information from the GitHub repo. This assumes that the GitHub repo is named as the &amp;ldquo;origin&amp;rdquo; remote (which is the default).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git fetch origin
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can then view the possible branches using&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git branch -r
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">biomass&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">function&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">fix&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">test&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">hao&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">vignette&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">hao&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="k">export&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">obs&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="k">func&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">hao&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">loadData&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">update&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">hao&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">reorder&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">remove&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">incomplete&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">censuses&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">master&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">namespace_issue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">namespaceissues&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">origin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">standardize_column_names&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We want to create a local branch to mirror the &amp;ldquo;fix-test&amp;rdquo; branch:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">~/projects/portalr &amp;gt; git checkout -b fix-test origin/fix-test
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Branch fix-test set up to track remote branch fix-test from origin.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Switched to a new branch &amp;#39;fix-test&amp;#39;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This has done several things: it retrieved the branch from GitHub to our local machine, set up tracking, and changed the current active branch. Now, if we make new commits to the local copy of the branch, we are able to push directly to that corresponding branch on GitHub.&lt;/p>
&lt;h2 id="pull-requests">Pull Requests&lt;/h2>
&lt;p>The preference is to use GitHub to merge the updates on a new branch back into &lt;code>master&lt;/code>. We can do this by going to the &amp;ldquo;Pull requests&amp;rdquo; tab on the GitHub repo page and creating a &amp;ldquo;New pull request&amp;rdquo;.
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/github_PR_tab.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Suppose we want to merge from &lt;code>hao-test-branch&lt;/code> into &lt;code>master&lt;/code>. Then we select &lt;code>master&lt;/code> as the &amp;ldquo;base: &amp;quot; branch, and &lt;code>hao-test-branch&lt;/code> as the &amp;ldquo;compare: &amp;quot; branch. We can then write some comments for our new pull request before clicking on &amp;ldquo;Create new pull request&amp;rdquo;.&lt;/p>
&lt;p>&lt;em>If the pull request fixes an issue, you can include keywords to
the issue when the pull request is merged.&lt;/em>&lt;/p>
&lt;h3 id="updating-pull-requests">Updating Pull Requests&lt;/h3>
&lt;p>At this point, other people can comment on the pull request itself in GitHub, if discussion regarding the changes needs to occur.&lt;/p>
&lt;p>Additionally, assuming that the pull request has not yet been merged, further commits &lt;em>to that branch on GitHub&lt;/em> are automatically included with the pull request. Thus, if you later find a bug, you can make further changes and not have to submit a new pull request.&lt;/p>
&lt;h3 id="merging-pull-requests">Merging Pull Requests&lt;/h3>
&lt;p>In general, check with one of the repo maintainers about merging pull requests. This ensures that the &lt;code>master&lt;/code> branch doesn&amp;rsquo;t break (too often) and that everyone is informed about changes.&lt;/p>
&lt;h2 id="summary-example">Summary Example&lt;/h2>
&lt;p>Objective: I want to fix issue #1 in the
repo.&lt;/p>
&lt;ol>
&lt;li>Download the repo from GitHub and onto my local machine. [&lt;code>git clone&lt;/code>]&lt;/li>
&lt;li>In my local machine, create a new branch (e.g. &lt;code>hao-add-biomass-function&lt;/code> &amp;lt;- prefacing the branch name with your name helps prevent branch name collisions. [&lt;code>git branch&lt;/code>]&lt;/li>
&lt;li>Switch to the new branch. [&lt;code>git checkout&lt;/code>]&lt;/li>
&lt;li>Make the updates on my local machine. [&lt;code>git commit&lt;/code>]&lt;/li>
&lt;li>Push the updates to GitHub. [&lt;code>git push&lt;/code>]&lt;/li>
&lt;li>Create the pull request on GitHub. [GitHub web interface]&lt;/li>
&lt;li>Merge the pull request on GitHub. [GitHub web interface]&lt;/li>
&lt;li>On my local machine, switch back to the master branch. [&lt;code>git branch&lt;/code>]&lt;/li>
&lt;li>Get the updates to the master branch [&lt;code>git pull&lt;/code>]
(optionally) Delete the branch on GitHub. [GitHub web interface, &amp;ldquo;Code&amp;rdquo; tab, &amp;ldquo;## branches&amp;rdquo;]
(optionally) Delete the branch on my local machine. [&lt;code>git branch -d hao-add-biomass-function&lt;/code>]&lt;/li>
&lt;/ol></description></item><item><title>Git Tips</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/git-tips/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/git-tips/</guid><description>&lt;h2 id="deleting-all-merged-branches-locally">Deleting all merged branches (locally)&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">git switch main
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">git branch --merged &lt;span class="p">|&lt;/span> egrep -v &lt;span class="s2">&amp;#34;(^\*|master|main|dev)&amp;#34;&lt;/span> &lt;span class="p">|&lt;/span> xargs git branch -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Source:
&lt;/p>
&lt;h2 id="pretty-git-log">Pretty git Log&lt;/h2>
&lt;p>(Hao) I have this in my ~/.gitconfig file to enable the git lg alias on command-line.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[alias]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lg1 = log --graph --abbrev-commit --decorate --format=format:&amp;#39;%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%&amp;lt;(40,trunc)%s%C(reset) %C(reverse white)- %an%C(reset)%C(bold yellow)%d%C(reset)&amp;#39; --all
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lg2 = log --graph --abbrev-commit --decorate --format=format:&amp;#39;%C(bold blue)%h%C(reset) - %C(bold cyan)%aD%C(reset) %C(bold green)(%ar)%C(reset)%C(bold yellow)%d%C(reset)%n&amp;#39;&amp;#39; %C(white)%s%C(reset) %C(dim white)- %an%C(reset)&amp;#39; --all
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lg = !&amp;#34;git lg1&amp;#34;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>GitHub Actions</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/github-actions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/github-actions/</guid><description>&lt;h2 id="generic-tutorial">Generic Tutorial&lt;/h2>
&lt;p>This is an example tutorial on how to set up a GitHub Action workflow.
In case the project has a Travis file, most of work would be copying and pasting the command into the different stages of the online template that is provided by GitHub.&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=F3wZTDmHCFA" target="_blank" rel="noopener">https://www.youtube.com/watch?v=F3wZTDmHCFA&lt;/a>&lt;/p></description></item><item><title>Globus for file transfer</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/globus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/globus/</guid><description>&lt;p>&lt;a href="https://app.globus.org" target="_blank" rel="noopener">Globus&lt;/a> is a useful file manager for transferring files between your computer and the HiPerGator, or from place to place on the HiPerGator.&lt;/p>
&lt;p>&lt;a href="https://help.rc.ufl.edu/doc/Globus" target="_blank" rel="noopener">Here is the HiPerGator guide to Globus.&lt;/a> It may be more up to date.&lt;/p>
&lt;p>Briefly, to use Globus, you need to create an account with your UF login. You may need to get authorization to be added to the UF research computing access. Once you have access, you can use the online Globus interface to set up file transfers from different locations (called &amp;ldquo;endpoints&amp;rdquo;) on the HiPerGator.&lt;/p>
&lt;p>To transfer files to and from your computer, you need to install the Globus Connect Personal account on your computer and set it up as an &amp;ldquo;endpoint&amp;rdquo;. Then you can also set up transfers to and from your computer.&lt;/p></description></item><item><title>Headings</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/headings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/grant-writing/headings/</guid><description>&lt;h2 id="why-use-headings">Why use headings?&lt;/h2>
&lt;p>Headings are the formal way of indicating different sections in a document.
This applies to everything from papers written in Word or Google Docs to webpages to software documentation.&lt;/p>
&lt;p>Proper headings are more than just visual indicators of sections.
Using them provides:&lt;/p>
&lt;ol>
&lt;li>Accessibility - screen readers know about headings and how to navigate with them. This lets folks using screen readers easily navigate the document. They can&amp;rsquo;t do this using bold or capitalized text.&lt;/li>
&lt;li>Easier navigation for everyone - as with many improvements to access headings make everyone&amp;rsquo;s life easier. Using headings makes it possible to add tables of contents and these are often auto generate by systems like Word, Google Docs, pdfs, and websites (see that nice table on contents on the right side of this webpage? That&amp;rsquo;s thanks to the headings!). This makes it much easier for everyone to navigate jump around in the document. It also lets you link directly to individual sections (e.g., to ask a collaborate to comment on a subsection of the Methods or direct users to part of a webpage)&lt;/li>
&lt;li>It&amp;rsquo;s easier to keep consistent heading formatting. Manually formatting headings means that if you decide to change the heading style in one place you have to manually change it in all of the other places. Using proper headings let&amp;rsquo;s you just change it once and it will show up everywhere.&lt;/li>
&lt;/ol>
&lt;h2 id="how-to-use-headings">How to use headings?&lt;/h2>
&lt;h3 id="google-docs">Google Docs&lt;/h3>
&lt;h4 id="creating-a-heading">Creating a heading&lt;/h4>
&lt;ol>
&lt;li>In the main bar click on the dropdown that says &lt;code>Normal text&lt;/code> and select the heading level you want to use&lt;/li>
&lt;li>Type your heading&lt;/li>
&lt;li>Hit enter and the next line will be set back to normal text&lt;/li>
&lt;/ol>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://ufhcc-bcbsr.github.io/wiki/docs/publishing/headings-example-google-docs.gif" alt="Animated gif of: 1) clicking the &amp;lsquo;Normal text&amp;rsquo; dropbox in a Google Doc; 2) selecting &amp;lsquo;Heading 1&amp;rsquo;; 3) typing in the word &amp;lsquo;Introduction&amp;rsquo;; and 4) the cursor moves to the next line and the dropdown returns to displaying &amp;lsquo;Normal text&amp;rsquo; and the word &amp;lsquo;Introduction&amp;rsquo; appears in the outline bar on the left side" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A handy way to make sure you&amp;rsquo;ve picked the right heading level is to check the outline bar to make sure your new heading is nested appropriately.&lt;/p>
&lt;h4 id="changing-the-visual-formatting-for-a-heading">Changing the visual formatting for a heading&lt;/h4>
&lt;ol>
&lt;li>Highlight a heading of the level that you want to change&lt;/li>
&lt;li>Change the format as desired by changing fonts, font sizes, bold, italics, etc.&lt;/li>
&lt;li>Click the heading dropbox&lt;/li>
&lt;li>Hover over the current heading level&lt;/li>
&lt;li>Select &amp;lsquo;Update Heading to match&amp;rsquo;&lt;/li>
&lt;/ol>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://ufhcc-bcbsr.github.io/wiki/docs/publishing/change-heading-format-google-docs.gif" alt="Animated gif of: 1) highlighting the word &amp;lsquo;Introduction&amp;rsquo; in a Google Doc; 2) clicking the - sign next to the font size to decrease the size of &amp;lsquo;Introduction&amp;rsquo;; 3) selecting &amp;lsquo;Heading 1&amp;rsquo; from the main bar; 4) hovering of &amp;lsquo;Heading 1&amp;rsquo; in the list of headings; 5) selecting &amp;lsquo;Update Heading 1 to match&amp;rsquo;" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>HiPerGator Intro Guide</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/hipergator-intro-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/hipergator-intro-guide/</guid><description>&lt;h1 id="so-you-want-to-run-your-r-or-python-script-on-the-hipergator">So you want to run your R or python script on the HiperGator&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>This guide gives a high level overview of how one goes about running R or python scripts on a high performance cluster (HPC). There are no coding examples here, and instead is designed to give someone a frame of reference for how to approach things, and where other more detailed tutorials fit in the larger picture. The expected user is someone who is comfortable doing analysis and writing scripts in R using RStudio, or python with various IDEs, and has no HPC experience.&lt;/p>
&lt;p>This is written for users of the &lt;a href="https://www.rc.ufl.edu/get-started/hipergator/" target="_blank" rel="noopener">UFL HiperGator&lt;/a> who code in R or python, but most information will apply to potential users for any HPC system and with any scripting language.&lt;/p>
&lt;h2 id="hpc-use-cases">HPC Use cases&lt;/h2>
&lt;p>There are two scenarios where you may want to run your analysis script on the HPC.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Your code takes a very long time to run&lt;/strong>&lt;br>
If your code is taking several hours, days or more to run on your personal computer then using an HPC is likely a good option for two reasons. One is the servers on an HPC have processors much faster than desktops and laptops, so with minimal changes your code will run significantly faster. Second is scripts on an HPC run independently of your personal computer, so you can shutdown your personal computer while the script on the HPC runs overnight or over the weekend. HPC systems can have a time limit of several weeks to a month for any single job.&lt;br>
If your script takes so long that it seems like it will never finish then an HPC can be especially beneficial. Say you do a test run on 1% of your data and it takes 2 days to run. Theoretically it will then take 200 days to run on your full dataset. In this case the benefit of the HPC is its parallel processing power.&lt;br>
By default R and python scripts run on a single processor. Most computers today have 4-8 processors though. And HPC servers have upwards of 64. If you spread the work out to multiple processors you can significantly decrease the amount of time it takes to run. For example: a script that takes 1 hour to run can potentially take 0.5 hours with 2 processors, or 15 minutes with 4 processors, or 7.5 minutes with 4 processors, and so on. There is computational overhead with parallel processing though so halving the time with a doubling of the number of processors is only a general rule. This is not a straightforward change to your code though and it will take time. See below about making scripts parallel and whether its even worth it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Your script fills up your computer&amp;rsquo;s memory and crashes when it runs.&lt;/strong>&lt;br>
When you have large datasets, such as raster files, its easy to use up all the memory and freeze your computer. As opposed to just waiting on long running scripts, in this case it makes it nearly impossible to do analysis. Just like HPC servers have powerful processors, they also have extremely large amounts of memory. Usually greater than 100GB. There is a good chance that they can handle whatever large datasets you throw at them.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="should-i-bother-with-an-hpc">Should I bother with an HPC?&lt;/h2>
&lt;p>Your analysis and data can be of &lt;em>any&lt;/em> size. There is no minimum computational requirement to use an HPC. But understand there is a time cost involved with learning how to interact with an HPC and also optimizing your code so it runs most efficiently. Therefore, in some cases it isnt worth it porting scripts to an HPC system.&lt;/p>
&lt;p>Consider an example where you have a script that takes 1 hour on your laptop, and you must run it once a month. Its likely reasonable to just keep that workflow. But if a script takes 10 hours and you must run it once a week, then its worth considering doing it on an HPC. Especially since that will decrease the wear and tear on your laptop and free up that 10 hours for other uses.&lt;/p>
&lt;p>Whether its worth it or not is unique to every situation. Also remember that once you learn all the HPC basics the first time, then that time cost isn&amp;rsquo;t needed for your next project.&lt;/p>
&lt;p>Also consider that the two use cases described above might also be solvable by code optimization. If you can find a section of code which is slow and make it run fast enough to meet your needs, that is preferable over running the code on an HPC. There is no one solution to this, but a good starting point is Hadley Wickhams Advanced R tutorial on &lt;a href="http://adv-r.had.co.nz/Performance.html" target="_blank" rel="noopener">Performance and Profiling&lt;/a>. This &lt;a href="https://youtu.be/K_90QGUPYCA" target="_blank" rel="noopener">45 minute video&lt;/a> also gives a great overview of profiling, optimization, parallel processing, and the implications in R.&lt;/p>
&lt;h2 id="what-exactly-is-an-hpc">What exactly is an HPC?&lt;/h2>
&lt;p>A high performance cluster (HPC) is primarily two things.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Its hundreds of individual servers in a data center. Each server is a computer just like your personal computer, but has more powerful components, and does not have a graphical user interface or even a monitor. You interact with the servers via the command line. If youve never used the command line consider it like the Rstudio console, or a python prompt, was your &lt;em>only&lt;/em> way to interact with a computer. More on this below.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Its a system for scheduling, prioritizing, and running scripts from hundreds of users. This is how the hundreds of servers can be used as one. Access to them is controlled by scheduling programs which you interact with, which then put your scripts in a queue to be run when resources are free. Slurm is probably the most popular scheduler (and the one used on the HiperGator) but some HPC systems may use other ones like &lt;a href="https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters" target="_blank" rel="noopener">PBS or MOAB&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="primary-steps-to-running-your-code-on-an-hpc">Primary Steps to running your code on an HPC.&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>You need an account&lt;/strong>.&lt;br>
Signup for HiperGator HPC account &lt;a href="https://www.rc.ufl.edu/access/account-request/" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>You must be able to login to the HPC via SSH and use the command line.&lt;/strong>&lt;br>
The command line can also be referred to as the unix shell. With this you use text commands (just like the RStudio console) to copy files, edit text files, interact with the scheduler, view job status, etc. See the &lt;a href="https://help.rc.ufl.edu/doc/Getting_Started#Connecting_to_HiPerGator" target="_blank" rel="noopener">Hipergator Connection guide&lt;/a>.&lt;/p>
&lt;p>Some unix tutorials:
- Scinet &lt;a href="https://geospatial.101workbook.org/IntroductionToCommandLine/Unix/unix-basics-1.html" target="_blank" rel="noopener">Geospatial Unix Intro&lt;/a>
- Data Carpentry &lt;a href="https://swcarpentry.github.io/shell-novice/" target="_blank" rel="noopener">tutorial on the unix shell&lt;/a>&lt;/p>
&lt;p>If you work on a Mac computer you have a full unix shell available already called the Terminal. For Windows users there are several options available. See the bottom of the Setup page for the Data Carpentry unix shell tutorial.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>You must optimize your code to run on the HPC.&lt;/strong>&lt;br>
This is potentially the trickiest part. At a minimum your code must be able to run independently without any interaction from you. Do you have one large (or even several) analysis script where you highlight different parts to run in the correct order? Or to check output before moving on? That will not work on an HPC. A single R or python file (aka a script) must run from start to finish and write some results to a file to be able to be useful on the HPC.&lt;/p>
&lt;p>For R, a good test for this is the Jobs tab in RStudio (next to Console and Terminal tabs. Not available on older versions of RStudio). This is &lt;em>very&lt;/em> analogous to running a script in an HPC environment. If your script can run as an RStudio Job &lt;em>without&lt;/em> copying the local environment or copying the job results anywhere (your script should write results to some file) then it should be able to run on the HPC.&lt;/p>
&lt;p>For python a good test is being able to run the script via the command line (ie. &lt;code>python my_analysis.py&lt;/code>). If you are using an IDE (like spyder or pycharm) then running a full script from start to finish using the Run option should also be sufficient.&lt;/p>
&lt;p>Having a script run without any interaction does not necessarily mean it needs to have parallel processing. See Should I make my code run parallel? below.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>You have to get your code and data onto the HPC.&lt;/strong>&lt;br>
Youll need to use special programs to transfer files (both data and scripts) from your local computer to the HPC. For windows this will be the WinScp program, which will have the same username and password as logging into the command line. For mac or linux users you can use the Terminal to transfer files via the command line using the scp command. Read more about the &lt;a href="https://scinet.usda.gov/guides/data/datatransfer#small-data-transfer-using-scp-and-rsync" target="_blank" rel="noopener">scp command here&lt;/a>. More on data transfer for HiperGator can be found here. Something youll see mentioned a lot is Globus, which is a useful (but not strictly required) tool when you need to transfer 100GB+ of data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>You need to ensure you have the correct packages.&lt;/strong>&lt;br>
Most HPC systems will have common packages installed and ready to use. If not youll have to install them yourself. If you do this then the latest versions will be installed on the HPC, so its good practice to make sure all packages on your personal computer are up to date to so they match (in RStudio use Tools-&amp;gt; Check for package updates, in python use conda or pip to update all package to the latest version).
For python packages for your projects youll want to use environments with either conda or python virtual environments.&lt;/p>
&lt;p>If you run into errors installing R or python packages you&amp;rsquo;ll likely need to contact HPC support for help, especially if the errors involve missing system libraries. If you successfully install your own packages they will only be available to you, and not to anyone else using the HPC.&lt;/p>
&lt;p>Also take note of the &lt;a href="https://help.rc.ufl.edu/doc/Modules_Basic_Usage" target="_blank" rel="noopener">module&lt;/a> command on the HiperGator. This is used to load preloaded software, including R and python themselves. This is covered is most tutorials about batch scripts (see next section).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>You can now submit your scripts to the scheduler.&lt;/strong>&lt;br>
Once your data and scripts are on the HPC system you can submit them to the scheduler to run. This involves defining jobs where you tell the scheduler what you need. Specifically: the location of your script, the resources needed (cpus and memory), the time needed to run your script, the location for putting script output and logs, etc.&lt;/p>
&lt;p>Jobs are defined via batch scripts which have a line for each piece of information.&lt;/p>
&lt;p>Some examples:&lt;/p>
&lt;ul>
&lt;li>A &lt;a href="https://geospatial.101workbook.org/Workshops/2-Session2-intro-to-ceres.html#batch-computing-on-ceres" target="_blank" rel="noopener">USDA tutorial on batch scripts&lt;/a>.&lt;/li>
&lt;li>A &lt;a href="https://help.rc.ufl.edu/doc/Sample_SLURM_Scripts" target="_blank" rel="noopener">sample of Hipergator batch scripts&lt;/a>.&lt;/li>
&lt;li>An &lt;a href="https://help.rc.ufl.edu/doc/Annotated_SLURM_Script" target="_blank" rel="noopener">annotated Hipergator batch script&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>You might need to debug your script if it doesnt run correctly.&lt;/strong>&lt;br>
Its very common for scripts to not run the first time because they were written on a personal computer and things like directory paths and packages may be different. In this case its useful to debug your script on the HPC directly.
A good place to do this is an interactive node or interactive session. For these instead of submitting a job to the queue, you request a new unix shell where a small amount of resources are available. Here you can run your scripts via the Rscript or python command and see the output directly, and make adjustments as needed until it runs successfully.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://help.rc.ufl.edu/doc/Development_and_Testing" target="_blank" rel="noopener">HiperGator guide on interactive sessions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://help.rc.ufl.edu/doc/GPU_Access#Interactive_Access" target="_blank" rel="noopener">HiperGator guide on interactive sessions when using GPUs&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>You can now get your results back.&lt;/strong>&lt;br>
This is the same process as putting scripts and data onto the HPC but in reverse.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="should-i-make-my-code-run-on-parallel-processes">Should I make my code run on parallel processes?&lt;/h2>
&lt;p>Before you dive into making your script parallel, do a quick cost/benefit analysis. It may indeed take a full day or more to redo your code to take advantage of parallel processing, but the benefits could be extremely large. If your code already runs in a relatively short time, like a few hours on your laptop and less than an hour on the HPC without any modification, and you&amp;rsquo;re happy with that then making it parallel might not be worth it.&lt;/p>
&lt;p>If you do not use parallel processing then your jobs will always request just a single processor. This is perfectly fine as there is no minimum requirement for using an HPC.&lt;/p>
&lt;h2 id="make-your-scripts-use-parallel-processing">Make your scripts use parallel processing&lt;/h2>
&lt;p>By default R and python run on a single processor. Most computers today have 4-8 processors. If you spread the work out to multiple processors you can significantly decrease the amount of time it takes to run.
For example: a script that takes 1 hour to run can potentially take 30 minutes with 2 processors, or 15 minutes with 4 processors. To make your scripts run across multiple processors, you&amp;rsquo;ll have to make some adjustments to your code.&lt;/p>
&lt;p>For R users, if your code uses lapply to run your main function to many items (e.g. fitting the same model to many species), you can swap it for mclapply from the parallel package without making any substantial changes. For more details and advanced uses, here are some short tutorials that go over this:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beckmw.wordpress.com/2014/01/21/a-brief-foray-into-parallel-processing-with-r/" target="_blank" rel="noopener">A brief foray into parallel processing with R&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://resbaz.github.io/r-intermediate-gapminder/19-foreach.html" target="_blank" rel="noopener">Software Carpentry Parallel Processing in R&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://swcarpentry.github.io/python-intermediate-mosquitoes/04-multiprocessing.html" target="_blank" rel="noopener">Software Carpentry Parallel Processing in python&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Some notes:&lt;/p>
&lt;ul>
&lt;li>If your code already uses functions and for loops, it should be straightforward to make it parallel, unless each pass through the loop depends on the outcome from previous passes.&lt;/li>
&lt;li>On your own computer, never set the amount of processors used to the max available. This will take away all the processing power needed to run the operating system, browser, and other programs, and could potentially crash your computer. To test out parallel code on my computer I set the number of processors to use at 2 (out of 8 available). Then when the scripts are moved to the HPC I set the amount to something higher.&lt;/li>
&lt;/ul>
&lt;h2 id="what-about-distributed-computing">What about distributed computing?&lt;/h2>
&lt;p>The links and examples for parallel computing above show you how to utilize the multiple processors in a single system. In the case of the HPC this means up to (usually) 64-128 processors on a single server. But what if you still need more processing power? In this case its possible to write parallel code which takes advantages of the processors on &lt;em>numerous&lt;/em> individual servers. This is how one utilizes 100s or even 1000s of processors.&lt;/p>
&lt;p>Going from single system parallel processing to distributed computing with your script is possible but will likely take even more work on your part. For this you might come across tutorials using MPI technology. MPI (Message-Passing Interface) is the protocol for analysis scripts to communicate between servers in an HPC environment, thus enabling distributed computing. Packages to use MPI are available in all common languages such as R, python, julia, and matlab.&lt;/p>
&lt;p>Newer packages are available which either deal with MPI in the background for you, or implement newer protocols. The R package &lt;a href="https://mllg.github.io/batchtools/index.html" target="_blank" rel="noopener">batchtools&lt;/a> has many high end functions for distributed computing. The python package &lt;a href="https://docs.dask.org/" target="_blank" rel="noopener">dask&lt;/a> is a state of the art package for distributed computing, and the accompanying &lt;a href="https://jobqueue.dask.org" target="_blank" rel="noopener">jobqueue&lt;/a> package integrates it with SLURM and other HPC schedulers.&lt;/p>
&lt;h2 id="other-considerations-and-important-points">Other considerations and important points&lt;/h2>
&lt;p>The Research Computing group has a &lt;a href="https://help.rc.ufl.edu/doc/UFRC_Help_and_Documentation" target="_blank" rel="noopener">wiki on HiperGator usage&lt;/a>.&lt;/p>
&lt;p>Some common tasks and scripts are outlind in the &lt;a href="https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/hipergator-reference/">HiPerGator Reference Guide&lt;/a>&lt;/p>
&lt;p>&lt;strong>Login Node&lt;/strong>: When you sign into the HPC there is a single landing server which youll start on. Its important to never run actual scripts on this initial server. It should be used to submit jobs, request development nodes or interactive sessions, or transfer data in or out.&lt;/p>
&lt;p>&lt;strong>Partitions&lt;/strong>: HiperGator resources are divided into partitions, where each partition has a specific set of hardware and resource and tim**e limits. Whenever you request resources youll specify which partition you want to use. See the &lt;a href="https://help.rc.ufl.edu/doc/Available_Node_Features" target="_blank" rel="noopener">partitions wiki page&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Account limits&lt;/strong>: The resources you request (eg. number of processors and amount of memory) is limited by how many credits your group has purchased. The number of jobs which can be running concurrently is also determined by this. See more on the &lt;a href="https://help.rc.ufl.edu/doc/Account_and_QOS_limits_under_SLURM" target="_blank" rel="noopener">account limits wiki page&lt;/a>. This is also referred to as QOS (quality of service), a term coined in the &lt;a href="https://en.wikipedia.org/wiki/Quality_of_service" target="_blank" rel="noopener">early internet days&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Processors/CPU/Cores/Sockets/Threads&lt;/strong>: Each of these things is technically different and has a distinct definition. For most users of an HPC system they can be thought of interchangeably though. When you request resources via a batch script, or other method, youll usually ask for multiple CPUs to implement parallel processing and leave it at that. Advanced users can read about different terminology &lt;a href="https://login.scg.stanford.edu/faqs/cores/" target="_blank" rel="noopener">here&lt;/a> or &lt;a href="https://slurm.schedmd.com/mc_support.html" target="_blank" rel="noopener">here&lt;/a>.&lt;/p></description></item><item><title>HiPerGator Reference</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/hipergator-reference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/hipergator-reference/</guid><description>&lt;h2 id="what-is-hipergator">What is HiperGator?&lt;/h2>
&lt;p>A University of Florida super-computing cluster.&lt;/p>
&lt;h2 id="why-should-i-use-it">Why should I use it?&lt;/h2>
&lt;p>HiperGator gives the user access to very large processing/memory/storage. This is useful for projects which can&amp;rsquo;t be run on your local laptop.&lt;/p>
&lt;h2 id="how-do-i-access-it">How do I access it?&lt;/h2>
&lt;ol start="0">
&lt;li>
&lt;p>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Connect with &lt;code>ssh &amp;lt;YOUR_USERNAME&amp;gt;@hpg2.rc.ufl.edu&lt;/code> from the Unix terminal or a Windows SSH client (
). Enter your password when prompted.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Need help with command line? A good tutorial is available at
.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="s=100" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;img src="https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/hipergator-login.png" height="400">
&lt;h2 id="how-do-i-run-a-job">How do I run a job?&lt;/h2>
&lt;p>For large analysis, you should submit a &lt;em>batch script&lt;/em> that tells Hipergator how to run your code. Let&amp;rsquo;s look at an example and walk through it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="cp">#!/bin/bash
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Job name and who to send updates to&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --job-name=&amp;lt;JOBNAME&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --mail-user=&amp;lt;EMAIL&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --mail-type=FAIL,END&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --account=ewhite&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --partition=hpg2-compute&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --qos=ewhite-b # Remove the `-b` if the script will take more than 4 days; see &amp;#34;bursting&amp;#34; below&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Where to put the outputs: %j expands into the job number (a unique identifier for this job)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --output my_job%j.out&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --error my_job%j.err&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Number of nodes to use&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --nodes=1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Number of tasks (usually translate to processor cores) to use: important! this means the number of mpi ranks used, useless if you are not using Rmpi)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --ntasks=1 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#number of cores to parallelize with:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --cpus-per-task=15&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --mem=16000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Memory per cpu core. Default is megabytes, but units can be specified with M &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># or G for megabytes or Gigabytes.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --mem-per-cpu=2G&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Job run time in [DAYS]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># HOURS:MINUTES:SECONDS&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># [DAYS] are optional, use when it is convenient&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --time=72:00:00&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Save some useful information to the &amp;#34;output&amp;#34; file&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">date&lt;span class="p">;&lt;/span>hostname&lt;span class="p">;&lt;/span>&lt;span class="nb">pwd&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Load R and run a script named my_R_script.R&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rscript my_R_script.R
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are successful, you&amp;rsquo;ll get a small message stating your job ID. Once your batch job is running, you can freely log out (or even turn off your local machine) and wait for an email telling you that it finished. You can log back in to see the results later.&lt;/p>
&lt;h2 id="interactive-work">Interactive work&lt;/h2>
&lt;h3 id="cpu">CPU&lt;/h3>
&lt;p>If you are running into errors, need to install a package in your local directory, or want to download some files, you should use a development server. This is good practice and nice to other people who are logged into the main head node.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#load module made by hipergator admin&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ml&lt;/span> &lt;span class="n">ufrc&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#request a server for 3 hours with 2GB of memory&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">srundev&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">time&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">00&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">00&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">mem&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="n">GB&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="gpu">GPU&lt;/h3>
&lt;p>To test out work involving a GPU you need to explicitly request a development node associated with a GPU. For many GPU tasks you may want a meaningful amount of memory.&lt;/p>
&lt;p>In most cases you&amp;rsquo;ll use the default GPUs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">srun --nodes&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span> --gpus&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span> --mem 20GB --cpus-per-task&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span> --pty -u bash -i
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But if you need a lot of VRAM (&amp;gt;24 GB/GPU) you can use the B200 nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">srun -p hpg-b200 --nodes&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span> --gpus&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span> --mem 50GB --cpus-per-task&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span> --pty -u bash -i
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To increase the number of GPUs increase the value of &lt;code>--gpu&lt;/code>, but you typically shouldn&amp;rsquo;t need more than 2 for interactive work and then only if you&amp;rsquo;re setting up multi-GPU testing.&lt;/p>
&lt;p>To increase the number of CPUs increase the value for &lt;code>--cpus-per-task&lt;/code>&lt;/p>
&lt;h2 id="how-do-i-know-if-its-running">How do I know if its running?&lt;/h2>
&lt;p>Use squeue -u &lt;username>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[b.weinstein@login3 ~]$ squeue -u b.weinstein
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 25666905 gpu DeepFore b.weinst R 22:29:49 1 c36a-s7
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 25672257 gpu DeepFore b.weinst R 21:07:19 1 c37a-s36
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The column labeled &amp;ldquo;S&amp;rdquo; is your job status. You want this to be &lt;code>R&lt;/code> for &amp;ldquo;running&amp;rdquo;, but it can spend a while as &lt;code>Q&lt;/code> (in the queue) before starting, especially if you request many cores. Sometimes (but not always) the output will explain why you&amp;rsquo;re still in the queue (e.g. QOSMEMLIMIT if you&amp;rsquo;re using too much memory).&lt;/p>
&lt;h2 id="how-do-i-get-my-data-on-to-hipergator">How do I get my data on to HiperGator?&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Often, the easiest way to transfer files to the server is using &lt;code>git clone&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If your files aren&amp;rsquo;t in a git repository, you can use FTP or &lt;code>scp&lt;/code>. FTP has graphical user interfaces that allow you to drag and drop files to the server.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you use &lt;code>scp&lt;/code>, the syntax for copying one file from your user folder on the server to your local folder is &lt;code>scp MY_USER_NAME@gator.hpc.ufl.edu:/home/MY_USER_NAME/PATH_TO_MY_FILE MY_LOCAL_FILENAME&lt;/code>. Note the space between the remote path and your local filename. If you want to send a file in the other direction, switch the order of the local file and the remote location. You can copy whole folders with the &lt;code>-r&lt;/code> flag.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If your files are large you should use Globus. See
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>
&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="storage">Storage&lt;/h2>
&lt;p>There are a few locations to store files on the hipergator&lt;/p>
&lt;p>&lt;code>/blue/ewhite/&lt;/code>&lt;br>
This is the primary space for storing large files, any large amounts of data generated by your programs should be pointed to this location.&lt;/p>
&lt;p>&lt;code>/orange/ewhite/&lt;/code>&lt;br>
This is another space to store large files. The total allocation here is much bigger than &lt;code>/blue&lt;/code>, but this storage space is slower than &lt;code>/blue&lt;/code>. If you have 100GB&amp;rsquo;s of data that you want to save, but are not currently using, &lt;code>/orange&lt;/code> is where they should go.&lt;/p>
&lt;p>&lt;code>/home/your_username/&lt;/code>&lt;br>
Your home directory has 20GB storage space for your scripts, logs, etc. You should not be storing large amounts of data here.&lt;/p>
&lt;h3 id="local-scratch-storage">Local scratch storage&lt;/h3>
&lt;p>&lt;code>$TMPDIR&lt;/code>&lt;br>
&lt;code>/blue&lt;/code> may be a bad place to store temporary cache files, especially if your program is generating 100&amp;rsquo;s of small (&amp;lt;1mb) files. An alternative is to use a temporary directory setup by SLURM every time you run a job. This can be referenced with the environment variable &lt;code>$TMPDIR&lt;/code> or &lt;code>SLURM_TMPDIR&lt;/code>. Read more about this here:
.&lt;/p>
&lt;p>This storage is available on each worker (but not login nodes) but does not persist. For example, it can be referenced from python&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">import os
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">os.env[&amp;#34;TMPDIR&amp;#34;]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To access this storage during a run, you can interactively ssh into the node and check out what&amp;rsquo;s there. The folder is named /scratch/local/{job_pid}.&lt;/p>
&lt;p>Note that for &lt;code>/blue&lt;/code> and &lt;code>/orange&lt;/code> if you are working on individual projects that are part of a larger effort you should work in a subdirectory &lt;code>/blue/ewhite/&amp;lt;your_username&amp;gt;/&lt;/code> and &lt;code>/orange/ewhite/&amp;lt;your_username&amp;gt;/&lt;/code>.&lt;/p>
&lt;p>Our current allocations as of July 2019&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Storage Type&lt;/th>
&lt;th>Location&lt;/th>
&lt;th>Quota&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>orange&lt;/td>
&lt;td>&lt;code>/orange&lt;/code>&lt;/td>
&lt;td>48 TB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>blue&lt;/td>
&lt;td>&lt;code>/blue&lt;/code>&lt;/td>
&lt;td>25 TB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>home&lt;/td>
&lt;td>&lt;code>/home/&amp;lt;your_username&amp;gt;&lt;/code>&lt;/td>
&lt;td>20 GB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="status">Status&lt;/h2>
&lt;p>You can check the status of HiPerGator using the
&lt;/p>
&lt;h2 id="best-practices">Best Practices&lt;/h2>
&lt;p>Below are a collection of best practices by past Weecology Users. These are not the only way to do things, just some useful tools that worked for us.&lt;/p>
&lt;h3 id="r">R&lt;/h3>
&lt;h4 id="installing-packages">Installing packages&lt;/h4>
&lt;p>HiPerGator has a lot of packages installed already, but you might need to install your own, or you might want an updated version of an existing package.&lt;/p>
&lt;p>You can tell R to prefer your personal library of R packages over the ones maintained for Hipergator by adding &lt;code>.libPaths(c(&amp;quot;/home/YOUR_USER_NAME/R_libs&amp;quot;, .libPaths()))&lt;/code> to your &lt;code>.Rprofile&lt;/code>. If you don&amp;rsquo;t have one yet, you can create a new file with that name and put it in your home directory (e.g. in &lt;code>/home/harris.d/.Rprofile&lt;/code>).&lt;/p>
&lt;p>The end result will look like this.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weinstein&lt;/span>&lt;span class="err">@&lt;/span>&lt;span class="n">dev1&lt;/span> &lt;span class="o">~&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">$&lt;/span> &lt;span class="n">cat&lt;/span> &lt;span class="o">~/.&lt;/span>&lt;span class="n">Rprofile&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">.&lt;/span>&lt;span class="n">libPaths&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/home/b.weinstein/R_libs&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">.&lt;/span>&lt;span class="n">libPaths&lt;/span>&lt;span class="p">()))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;.Rprofile loaded&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You will need to create the &lt;code>R_libs&lt;/code> directory using &lt;code>mkdir R_libs&lt;/code>.&lt;/p>
&lt;p>When you load R, you should see&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weinstein&lt;/span>&lt;span class="err">@&lt;/span>&lt;span class="n">dev1&lt;/span> &lt;span class="o">~&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">$&lt;/span> &lt;span class="n">ml&lt;/span> &lt;span class="n">R&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weinstein&lt;/span>&lt;span class="err">@&lt;/span>&lt;span class="n">dev1&lt;/span> &lt;span class="o">~&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">$&lt;/span> &lt;span class="n">R&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">R&lt;/span> &lt;span class="n">version&lt;/span> &lt;span class="mf">3.5&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2018&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">07&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">02&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">--&lt;/span> &lt;span class="s2">&amp;#34;Feather Spray&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Copyright&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">C&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="mi">2018&lt;/span> &lt;span class="n">The&lt;/span> &lt;span class="n">R&lt;/span> &lt;span class="n">Foundation&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">Statistical&lt;/span> &lt;span class="n">Computing&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Platform&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x86_64&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pc&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">linux&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gnu&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">64&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">bit&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">R&lt;/span> &lt;span class="n">is&lt;/span> &lt;span class="n">free&lt;/span> &lt;span class="n">software&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">comes&lt;/span> &lt;span class="n">with&lt;/span> &lt;span class="n">ABSOLUTELY&lt;/span> &lt;span class="n">NO&lt;/span> &lt;span class="n">WARRANTY&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">You&lt;/span> &lt;span class="n">are&lt;/span> &lt;span class="n">welcome&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">redistribute&lt;/span> &lt;span class="n">it&lt;/span> &lt;span class="n">under&lt;/span> &lt;span class="n">certain&lt;/span> &lt;span class="n">conditions&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Type&lt;/span> &lt;span class="s1">&amp;#39;license()&amp;#39;&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="s1">&amp;#39;licence()&amp;#39;&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">distribution&lt;/span> &lt;span class="n">details&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Natural&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="n">support&lt;/span> &lt;span class="n">but&lt;/span> &lt;span class="n">running&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">an&lt;/span> &lt;span class="n">English&lt;/span> &lt;span class="n">locale&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">R&lt;/span> &lt;span class="n">is&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="n">collaborative&lt;/span> &lt;span class="n">project&lt;/span> &lt;span class="n">with&lt;/span> &lt;span class="n">many&lt;/span> &lt;span class="n">contributors&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Type&lt;/span> &lt;span class="s1">&amp;#39;contributors()&amp;#39;&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">more&lt;/span> &lt;span class="n">information&lt;/span> &lt;span class="ow">and&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1">&amp;#39;citation()&amp;#39;&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="n">how&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">cite&lt;/span> &lt;span class="n">R&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="n">R&lt;/span> &lt;span class="n">packages&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">publications&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Type&lt;/span> &lt;span class="s1">&amp;#39;demo()&amp;#39;&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">some&lt;/span> &lt;span class="n">demos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;help()&amp;#39;&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">on&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">line&lt;/span> &lt;span class="n">help&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="ow">or&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1">&amp;#39;help.start()&amp;#39;&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">an&lt;/span> &lt;span class="n">HTML&lt;/span> &lt;span class="n">browser&lt;/span> &lt;span class="n">interface&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">help&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Type&lt;/span> &lt;span class="s1">&amp;#39;q()&amp;#39;&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">quit&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="s2">&amp;#34;.Rprofile loaded&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once this is set up, you can install or update packages the usual way (e.g. with &lt;code>install.packages&lt;/code> or &lt;code>devtools::install_github&lt;/code>).&lt;/p>
&lt;h2 id="re-writing-your-code-to-take-advantage-of-multiple-cores">Re-writing your code to take advantage of multiple cores.&lt;/h2>
&lt;p>By default R runs on a single processor. Most computers today have 4-8 processors. If you spread the work out to multiple processors you can decrease the amount of time it takes to run by significantly. For example: a script that takes 1 hour to run can potentially take 0.5 hours with 2 processors, or 15 minutes with 4 processors. To make your scripts run across multiple processors, you&amp;rsquo;ll have to make some slight adjustments to your code.&lt;/p>
&lt;p>If your code uses &lt;code>lapply&lt;/code> to run your main function to many items (e.g. fitting a model to each species), you can swap it for &lt;code>mclapply&lt;/code> from the &lt;code>parallel&lt;/code> package without making any substantial changes. For more details and advanced uses, here are 2 short tutorials that go over this:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>
&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Some quick notes:&lt;/p>
&lt;ul>
&lt;li>If your code already uses functions and for loops, it should be very easy to make it parallel, unless each pass through the loop depends on the outcome from previous passes.&lt;/li>
&lt;li>On your own computer, never set the amount of processors used to the max available. This will take away all the processing power needed to run the operating system, browser, and other programs, and could potentially crash your computer. To test out parallel code on my computer I set the number of processors to use at 2 (out of 8 available).&lt;/li>
&lt;/ul>
&lt;h3 id="batchtools">Batchtools&lt;/h3>
&lt;p>Recently, the R package batchtools has made simple parallel job submissions in R much easier. No more bash scripting, just submit a set of jobs by mapping a function to a list of inputs. Here is an example.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="n">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batchtools&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#Batchtools tmp registry&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">reg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">makeRegistry&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dir&lt;/span> &lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;.&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;registry created&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Toy function that just sleeps&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">fun&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">function&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Sys&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sleep&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">t&lt;/span>&lt;span class="o">&amp;lt;-&lt;/span>&lt;span class="n">Sys&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">paste&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;worker&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;time is&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#batchtools submission&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">reg&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functions&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">makeClusterFunctionsSlurm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">template&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;detection_template.tmpl&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">jobs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TRUE&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">nodename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;localhost&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scheduler&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">latency&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">latency&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">65&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ids&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">batchMap&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fun&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">seq&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">reg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">testJob&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">ids&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,],&lt;/span>&lt;span class="n">reg&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">reg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Set resources: enable memory measurement&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">walltime&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;2:00:00&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">memory&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;4GB&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Submit jobs using the currently configured cluster functions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">submitJobs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ids&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">resources&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">waitForJobs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ids&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">getStatus&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">getJobTable&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>with a SLURM template in the same directory.&lt;/p>
&lt;p>detection_template.tmpl&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="cp">#!/bin/bash
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Modified from https://github.com/mllg/batchtools/blob/master/inst/templates/&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Job Resource Interface Definition&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">##&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ntasks [integer(1)]: Number of required tasks,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Set larger than 1 if you want to further parallelize&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## with MPI within your job.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ncpus [integer(1)]: Number of required cpus per task,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Set larger than 1 if you want to further parallelize&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## with multicore/parallel within each task.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## walltime [integer(1)]: Walltime for this job, in seconds.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Must be at least 60 seconds for Slurm to work properly.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## memory [integer(1)]: Memory in megabytes for each cpu.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Must be at least 100 (when I tried lower values my&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## jobs did not start at all).&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">##&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Default resources can be set in your .batchtools.conf.R by defining the variable&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## &amp;#39;default.resources&amp;#39; as a named list.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;%
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># relative paths are not handled well by Slurm&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">log.file &lt;span class="o">=&lt;/span> fs::path_expand&lt;span class="o">(&lt;/span>log.file&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>!&lt;span class="s2">&amp;#34;ncpus&amp;#34;&lt;/span> %in% names&lt;span class="o">(&lt;/span>resources&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> resources&lt;span class="nv">$ncpus&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>!&lt;span class="s2">&amp;#34;walltime&amp;#34;&lt;/span> %in% names&lt;span class="o">(&lt;/span>resources&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> resources&lt;span class="nv">$walltime&lt;/span>&amp;lt;-&lt;span class="s2">&amp;#34;1:00:00&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>!&lt;span class="s2">&amp;#34;memory&amp;#34;&lt;/span> %in% names&lt;span class="o">(&lt;/span>resources&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> resources&lt;span class="nv">$memory&lt;/span> &amp;lt;- &lt;span class="s2">&amp;#34;5GB&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-%&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Job name and who to send updates to&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --mail-user=benweinstein2010@gmail.com&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --mail-type=FAIL,END&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --account=ewhite&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --partition=hpg2-compute&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --qos=ewhite-b # Remove the `-b` if the script will take more than 4 days; see &amp;#34;bursting&amp;#34; below&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --job-name=&amp;lt;%= job.name %&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --output=&amp;lt;%= log.file %&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --error=&amp;lt;%= log.file %&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --time=&amp;lt;%= resources$walltime %&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --ntasks=1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --cpus-per-task=&amp;lt;%= resources$ncpus %&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#SBATCH --mem-per-cpu=&amp;lt;%= resources$memory %&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;lt;%&lt;span class="o">=&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>!is.null&lt;span class="o">(&lt;/span>resources&lt;span class="nv">$partition&lt;/span>&lt;span class="o">))&lt;/span> sprintf&lt;span class="o">(&lt;/span>paste0&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;#SBATCH --partition=&amp;#39;&amp;#34;&lt;/span>, resources&lt;span class="nv">$partition&lt;/span>, &lt;span class="s2">&amp;#34;&amp;#39;&amp;#34;&lt;/span>&lt;span class="o">))&lt;/span> %&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;lt;%&lt;span class="o">=&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>array.jobs&lt;span class="o">)&lt;/span> sprintf&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;#SBATCH --array=1-%i&amp;#34;&lt;/span>, nrow&lt;span class="o">(&lt;/span>&lt;span class="nb">jobs&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span> %&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Initialize work environment like&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## source /etc/profile&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## module add ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">source&lt;/span> /etc/profile
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Export value of DEBUGME environemnt var to slave&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">DEBUGME&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;%&lt;span class="o">=&lt;/span> Sys.getenv&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;DEBUGME&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> %&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;%&lt;span class="o">=&lt;/span> sprintf&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;export OMP_NUM_THREADS=%i&amp;#34;&lt;/span>, resources&lt;span class="nv">$omp&lt;/span>.threads&lt;span class="o">)&lt;/span> -%&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;%&lt;span class="o">=&lt;/span> sprintf&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;export OPENBLAS_NUM_THREADS=%i&amp;#34;&lt;/span>, resources&lt;span class="nv">$blas&lt;/span>.threads&lt;span class="o">)&lt;/span> -%&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;%&lt;span class="o">=&lt;/span> sprintf&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;export MKL_NUM_THREADS=%i&amp;#34;&lt;/span>, resources&lt;span class="nv">$blas&lt;/span>.threads&lt;span class="o">)&lt;/span> -%&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Run R:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## we merge R output with stdout from SLURM, which gets then logged via --output option&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;submitting job&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">module load gcc/6.3.0 R gdal/2.2.1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#add to path&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rscript -e &lt;span class="s1">&amp;#39;batchtools::doJobCollection(&amp;#34;&amp;lt;%= uri %&amp;gt;&amp;#34;)&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>yields&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Submitting 10 jobs in 10 chunks using cluster functions &amp;#39;Slurm&amp;#39; ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[1] TRUE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Status for 10 jobs at 2019-05-21 14:43:03:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Submitted : 10 (100.0%)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -- Queued : 0 ( 0.0%)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -- Started : 10 (100.0%)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ---- Running : 0 ( 0.0%)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ---- Done : 10 (100.0%)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ---- Error : 0 ( 0.0%)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ---- Expired : 0 ( 0.0%)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="python">Python&lt;/h2>
&lt;h3 id="installing-python-packages">Installing Python Packages&lt;/h3>
&lt;ul>
&lt;li>ssh onto HiperGator&lt;/li>
&lt;li>Download the conda installer: &lt;code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&lt;/code>&lt;/li>
&lt;li>Run the installer: &lt;code>bash Miniconda3-latest-Linux-x86_64.sh&lt;/code>&lt;/li>
&lt;li>Answer &amp;lsquo;Yes&amp;rsquo; at the end of the install to have conda added to your &lt;code>.bashrc&lt;/code>&lt;/li>
&lt;li>Install packages using &lt;code>conda install package_name&lt;/code>&lt;/li>
&lt;li>Run &lt;code>conda activate&lt;/code> as the first step in your slurm script&lt;/li>
&lt;/ul>
&lt;h3 id="dask-parallelization">Dask Parallelization&lt;/h3>
&lt;p>Dask can be submitted through dask-jobqueue.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">#################
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> # Setup dask cluster
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> #################
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> from dask_jobqueue import SLURMCluster
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> from dask.distributed import Client, wait
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> num_workers = 10
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> #job args
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> extra_args=[
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#34;--error=/home/b.weinstein/logs/dask-worker-%j.err&amp;#34;,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#34;--account=ewhite&amp;#34;,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#34;--output=/home/b.weinstein/logs/dask-worker-%j.out&amp;#34;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cluster = SLURMCluster(
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> processes=1,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> queue=&amp;#39;hpg2-compute&amp;#39;,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cores=1,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory=&amp;#39;13GB&amp;#39;,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> walltime=&amp;#39;24:00:00&amp;#39;,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> job_extra=extra_args,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> local_directory=&amp;#34;/home/b.weinstein/logs/&amp;#34;, death_timeout=300)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> print(cluster.job_script())
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cluster.adapt(minimum=num_workers, maximum=num_workers)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> dask_client = Client(cluster)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> #Start dask
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> dask_client.run_on_scheduler(start_tunnel)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> futures = dask_client.map(&amp;lt;function you want to parallelize&amp;gt;, &amp;lt;list of objects to run&amp;gt;, &amp;lt;additional args here&amp;gt;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> wait(futures)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="connecting-through-jupyter-notebooks">Connecting through jupyter notebooks.&lt;/h3>
&lt;p>Its useful to be able to interact with hipergator, without having to rely solely on the terminal. Especially when dealing with large datasets, instead of prototyping locally, then pushing to the cloud, we can connect directly using a jupyter notebook.&lt;/p>
&lt;ul>
&lt;li>Log on to hipergator and request an interactive session.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">srun --ntasks=1 --cpus-per-task=2 --mem=2gb -t 90 --pty bash -i
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we have 90 minutes work directly on this development node.&lt;/p>
&lt;ul>
&lt;li>Create a juypter notebook&lt;/li>
&lt;/ul>
&lt;p>Load the python module&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="n">module&lt;/span> &lt;span class="nb">load&lt;/span> &lt;span class="n">python&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Start the notebook and get your ssh tunnel&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">import socket
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">import subprocess
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">host = socket.gethostname()
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">proc = subprocess.Popen([&amp;#39;jupyter&amp;#39;, &amp;#39;lab&amp;#39;, &amp;#39;--ip&amp;#39;, host, &amp;#39;--no-browser&amp;#39;])
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">print(&amp;#34;ssh -N -L 8888:%s:8888 -l b.weinstein hpg2.rc.ufl.edu&amp;#34; % (host))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If all went well it should look something like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weinstein&lt;/span>&lt;span class="err">@&lt;/span>&lt;span class="n">c27b&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">s2&lt;/span> &lt;span class="n">dask&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">jobqueue&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">$&lt;/span> &lt;span class="n">python&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Python&lt;/span> &lt;span class="mf">3.6&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mi">4&lt;/span> &lt;span class="o">|&lt;/span>&lt;span class="n">Anaconda&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Inc&lt;/span>&lt;span class="o">.|&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">default&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Jan&lt;/span> &lt;span class="mi">16&lt;/span> &lt;span class="mi">2018&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">18&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">19&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">GCC&lt;/span> &lt;span class="mf">7.2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="n">linux&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Type&lt;/span> &lt;span class="s2">&amp;#34;help&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;copyright&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;credits&amp;#34;&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="s2">&amp;#34;license&amp;#34;&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">more&lt;/span> &lt;span class="n">information&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">import&lt;/span> &lt;span class="n">socket&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">import&lt;/span> &lt;span class="n">subprocess&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">host&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">socket&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gethostname&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">proc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">subprocess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Popen&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="s1">&amp;#39;jupyter&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;lab&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;--ip&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">host&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;--no-browser&amp;#39;&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;ssh -N -L 8888:&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">:8888 -l b.weinstein hpg2.rc.ufl.edu&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">host&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ssh&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">L&lt;/span> &lt;span class="mi">8888&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">c27b&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">s2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ufhpc&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">8888&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">l&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weinstein&lt;/span> &lt;span class="n">hpg2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rc&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ufl&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">edu&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.776&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="n">The&lt;/span> &lt;span class="n">port&lt;/span> &lt;span class="mi">8888&lt;/span> &lt;span class="n">is&lt;/span> &lt;span class="n">already&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">use&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">trying&lt;/span> &lt;span class="n">another&lt;/span> &lt;span class="n">port&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.799&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="n">JupyterLab&lt;/span> &lt;span class="n">beta&lt;/span> &lt;span class="n">preview&lt;/span> &lt;span class="n">extension&lt;/span> &lt;span class="n">loaded&lt;/span> &lt;span class="n">from&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">home&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weinstein&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">miniconda3&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">envs&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pangeo&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">lib&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">python3&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mi">6&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">site&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">packages&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">jupyterlab&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.799&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="n">JupyterLab&lt;/span> &lt;span class="n">application&lt;/span> &lt;span class="n">directory&lt;/span> &lt;span class="n">is&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">home&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weinstein&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">miniconda3&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">envs&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pangeo&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">share&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">jupyter&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">lab&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.809&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="n">Serving&lt;/span> &lt;span class="n">notebooks&lt;/span> &lt;span class="n">from&lt;/span> &lt;span class="n">local&lt;/span> &lt;span class="n">directory&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">home&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weinstein&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">dask&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">jobqueue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.809&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="n">active&lt;/span> &lt;span class="n">kernels&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.809&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="n">The&lt;/span> &lt;span class="n">Jupyter&lt;/span> &lt;span class="n">Notebook&lt;/span> &lt;span class="n">is&lt;/span> &lt;span class="n">running&lt;/span> &lt;span class="n">at&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.809&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="n">http&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">//&lt;/span>&lt;span class="n">c27b&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">s2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ufhpc&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">8889&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="err">?&lt;/span>&lt;span class="n">token&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="n">c9c992a219e1e35ddd4cbe782d7f1f56c6680118b13053c&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.809&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="n">Use&lt;/span> &lt;span class="ne">Control&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">C&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">stop&lt;/span> &lt;span class="n">this&lt;/span> &lt;span class="n">server&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">shut&lt;/span> &lt;span class="n">down&lt;/span> &lt;span class="n">all&lt;/span> &lt;span class="n">kernels&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">twice&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">skip&lt;/span> &lt;span class="n">confirmation&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="n">C&lt;/span> &lt;span class="mi">17&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">11&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">29.811&lt;/span> &lt;span class="n">LabApp&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Copy&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">paste&lt;/span> &lt;span class="n">this&lt;/span> &lt;span class="n">URL&lt;/span> &lt;span class="n">into&lt;/span> &lt;span class="n">your&lt;/span> &lt;span class="n">browser&lt;/span> &lt;span class="n">when&lt;/span> &lt;span class="n">you&lt;/span> &lt;span class="n">connect&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">the&lt;/span> &lt;span class="n">first&lt;/span> &lt;span class="n">time&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">to&lt;/span> &lt;span class="n">login&lt;/span> &lt;span class="n">with&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="n">token&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">http&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">//&lt;/span>&lt;span class="n">c27b&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">s2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ufhpc&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">8889&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="err">?&lt;/span>&lt;span class="n">token&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="n">c9c992a219e1e35ddd4cbe782d7f1f56c6680118b13053c&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>see that line ssh&amp;hellip;, that is what we need to enter in our local laptop. It will ask for your login password&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">MacBook-Pro:~ ben$ ssh -N -L 8888:c27b-s2.ufhpc:8888 -l b.weinstein hpg2.rc.ufl.edu
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">b.weinstein@hpg2.rc.ufl.edu&amp;#39;s password:
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Don&amp;rsquo;t worry if it looks like it hangs, the tunnel is open! Go check it out.&lt;/p>
&lt;p>Opening your browner, go to localhost:8888&lt;/p>
&lt;p>and viola, we are navigating hipergator from the confines of our own laptop.&lt;/p>
&lt;h2 id="support">Support&lt;/h2>
&lt;p>
.&lt;/p>
&lt;p>Hipergator staff are here to support you. Our grant money pays their salary. They are friendly and eager to help. When in doubt, just ask.&lt;/p>
&lt;p>For more information on (job submission scripts)[https://wiki.rc.ufl.edu/doc/Annotated_SLURM_Script]&lt;/p>
&lt;h2 id="priority">Priority&lt;/h2>
&lt;p>The supercomputer is a shared resource, and the SLURM scheduler has to decide how to divvy it up. The method they use for deciding when it&amp;rsquo;s your turn to use a machine based on a metric called &amp;ldquo;FairShare.&amp;rdquo; You can see your FairShare number by typing &lt;code>sshare -U&lt;/code> in your hipergator terminal. A FairShare of 0.5 means you&amp;rsquo;ve been using exactly your share. Larger numbers mean you can use more, while smaller numbers mean you&amp;rsquo;re using more than your share and will be given lower priority.&lt;/p>
&lt;p>Your &amp;ldquo;usage&amp;rdquo; number is an exponentially-weighted moving average of the resources you&amp;rsquo;ve consumed, with a half-life of two weeks. So if you&amp;rsquo;ve &amp;ldquo;bursted&amp;rdquo; at 10x for a while, it might take a few weeks before you&amp;rsquo;re given decent priority again.&lt;/p>
&lt;p>A more comprehensive description of FairShare is available
.&lt;/p>
&lt;h2 id="bursting">Bursting&lt;/h2>
&lt;p>If your jobs will take less than 4 days, you can use &amp;ldquo;burst&amp;rdquo; mode, which provides &lt;em>ten times&lt;/em> as many cores and &lt;em>ten times&lt;/em> as much memory as the default mode. If you cannot burst, just remove the &lt;code>-b&lt;/code> from the line above about &lt;code>qos&lt;/code>. Note than if you are using burst your jobs will automatically be killed after 96 hours if they haven&amp;rsquo;t already finished.&lt;/p>
&lt;h2 id="current-usage">Current usage&lt;/h2>
&lt;p>To see the current usage by our group, as well as overall hipergator usage, use the command&lt;/p>
&lt;p>&lt;code>slurmInfo -pu&lt;/code>&lt;/p>
&lt;p>To see the total available resources use:&lt;/p>
&lt;p>&lt;code>sacctmgr show qos ewhite format=&amp;quot;Name%-16,GrpSubmit,MaxWall,GrpTres%-45&amp;quot;&lt;/code>&lt;br>
for the normal queue, and &lt;br>
&lt;code>sacctmgr show qos ewhite-b format=&amp;quot;Name%-16,GrpSubmit,MaxWall,GrpTres%-45&amp;quot;&lt;/code>&lt;br>
for the &amp;ldquo;burst&amp;rdquo; queue.&lt;/p>
&lt;h2 id="partitions">Partitions&lt;/h2>
&lt;p>The HiperGator consists of hundreds of servers. These a split up into several &amp;ldquo;partitions&amp;rdquo; for various reasons.&lt;/p>
&lt;p>Most of the time you can just use the defaults, but there are a few species partitions available that you might been to request:&lt;/p>
&lt;ul>
&lt;li>&lt;code>hpg-b200&lt;/code> - This is the partition to use for GPU jobs with large VRAM needs (&amp;gt;24 GB/GPU). You need to have paid for GPU access, which our lab has.&lt;/li>
&lt;li>&lt;code>bigmem&lt;/code> - This partitions consists of several servers with up to 1TB of memory. This is useful if you need a &lt;em>lot&lt;/em> of memory but still want to keep a script on a single server.&lt;/li>
&lt;li>&lt;code>hpg2-dev&lt;/code> - These are several servers for development purposes. When you use &lt;code>srundev&lt;/code> the jobs get sent here.&lt;/li>
&lt;/ul>
&lt;h3 id="selecting-a-partition">Selecting a partition&lt;/h3>
&lt;p>By default you&amp;rsquo;ll run jobs on the &lt;code>hpg2-compute&lt;/code> partitions. If you want to change it, edit the &lt;code>--partition&lt;/code> line in your job script, or use the &lt;code>-p&lt;/code> command in &lt;code>srun&lt;/code>.&lt;/p>
&lt;h2 id="cron-jobs---how-to-run-regularly-scheduled-jobs">Cron jobs - how to run regularly scheduled jobs&lt;/h2>
&lt;p>If you&amp;rsquo;re unfamiliar with cron jobs read
.&lt;/p>
&lt;h3 id="ssh-to-daemon">SSH to daemon&lt;/h3>
&lt;p>Cron jobs on the HPC need to be setup on a special machine called &lt;code>daemon&lt;/code>.
You can ssh there from the HPC using &lt;code>ssh daemon&lt;/code>.
After that you can use the usual &lt;code>crontab -e&lt;/code> to setup your cron job.&lt;/p>
&lt;h3 id="setting-path-for-cron-jobs">Setting PATH for cron jobs&lt;/h3>
&lt;p>For some reason the PATH isn&amp;rsquo;t properly set when running cron jobs, so you need to set it at the top of the crontab.
Add a line like this adding any additional paths you need (e.g., the location of your conda environments).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">PATH&lt;/span>&lt;span class="o">=&lt;/span>/opt/slurm/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/home/USERNAME/bin:/blue/ewhite/USERNAME/miniconda3/bin/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="check-if-running-on-hipergator">Check if running on HiPerGator&lt;/h2>
&lt;p>Sometimes it&amp;rsquo;s useful to have code execute one way on your local computer and another way on the HPC. For HiPerGator you can do this by checking the environmental variable HOSTNAME and looking to see if it contains &lt;code>ufhpc&lt;/code>. For example, in R&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">if (grepl(&amp;#34;ufhpc&amp;#34;, Sys.getenv(&amp;#34;HOSTNAME&amp;#34;))){
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> hipergator_run()
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">} else {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> local_run()
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are submitting via SLURM, the hostname will not contain &amp;ldquo;ufhpc&amp;rdquo; but the nodename will. So use this logic:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">nodename &amp;lt;- Sys.info()[&amp;#34;nodename&amp;#34;]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">if(grepl(&amp;#34;ufhpc&amp;#34;, nodename)) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> print(&amp;#34;I know I am on SLURM!&amp;#34;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="using-rstudio-on-the-hipergator">Using RStudio on the hipergator&lt;/h2>
&lt;p>See the main Wiki page here on running gui programs.
&lt;/p>
&lt;h2 id="using-vscode-on-hipergator">Using VSCODE on hipergator&lt;/h2>
&lt;p>Vscode is a great development environment for many languages (python, java, bash), and allows powerful integration with github copilot and other debugging tools. The docs on hipergator
at how to do this, but don&amp;rsquo;t make it clear how to check out a node and develop with those resources. We can use
to do this easily.&lt;/p>
&lt;p>Start by creating a SLURM script to get a development node. In this case, I want a GPU node.&lt;/p>
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="o">(&lt;/span>base&lt;span class="o">)&lt;/span> &lt;span class="o">[&lt;/span>b.weinstein@login11 ~&lt;span class="o">]&lt;/span>$ cat tunnel.sh
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#!/bin/bash&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --job-name=tunnel # Job name&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --mail-type=END # Mail events&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --mail-user=benweinstein2010@gmail.com # Where to send mail&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --account=ewhite&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --nodes=1 # Number of MPI ran&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --cpus-per-task=10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --mem=70GB&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --time=12:00:00 #Time limit hrs:min:sec&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --output=/home/b.weinstein/logs/tunnel.out # Standard output and error log&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --error=/home/b.weinstein/logs/tunnel.err&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#SBATCH --gpus=1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">module load vscode
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">XDG_RUNTIME_DIR&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">SLURM_TMPDIR&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="p">;&lt;/span> code tunnel
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>Submit the job and view the logs&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">(base) [b.weinstein@login11 ~]$ sbatch tunnel.sh
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">(base) [b.weinstein@login11 ~]$ cat /home/b.weinstein/logs/tunnel.out
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">*
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">* Visual Studio Code Server
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">*
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">* By using the software, you agree to
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license) and
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">*
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[2024-05-23 11:56:57] info Using Github for authentication, run `code tunnel user login --provider &amp;lt;provider&amp;gt;` option to change this.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">To grant access to the server, please log into https://github.com/login/device and use code 3390-CCD9
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>
&lt;p>Go to
and authenticate with the code. You will see the hipergator logs change and successfully connect.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Go to your local vscode instance, active the &amp;lsquo;remote explorer&amp;rsquo; extension and click on &amp;rsquo;tunnels&amp;rsquo;. You will see the hipergator tunnel listed.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;img width="972" alt="image" src="https://github.com/weecology/wiki/assets/1208492/6fc30817-5e84-4350-bebf-be6318ebbc69">
&lt;p>Success! Now you are the GPU node and can debug and run with those resources!&lt;/p></description></item><item><title>How To Work With Us</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/collaborating-with-us/who-we-are/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/collaborating-with-us/who-we-are/</guid><description>&lt;h2 id="submit-a-support-request-form">Submit a Support Request Form&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://cancer.ufl.edu/research/shared-resources/biostatistics-computational-biology-shared-resource/biostatistics-shared-resource-support-request-form/" target="_blank" rel="noopener">Online Support Requet Form&lt;/a>:&lt;/li>
&lt;/ul>
&lt;p>Fill out the support request form as best you can. The important thing is to get connected with us, we&amp;rsquo;ll learn much more about your project later.&lt;/p>
&lt;h2 id="schedule-a-meeting-with-us">Schedule a Meeting With Us&lt;/h2>
&lt;p>Once we receive your support request, we&amp;rsquo;ll send you an email to schedule a meeting. Initial meetings are usually about an hour long, during which we want to understand your project and the expectations of a future collaboration.&lt;/p>
&lt;h2 id="how-does-the-funding-work">How Does the Funding Work?&lt;/h2>
&lt;p>The BCB-SR is a group of bioinformatics scientists whose salaries are primarily funded by effort on the grants on which they collaborate. Because of this, the ideal scenario is that you identify the need for bioinformatics collaboration &lt;strong>before&lt;/strong> your grant is submitted so that there will be an appropriate allocation of funding. If this is not the case, we will estimate our effort given the project&amp;rsquo;s needs and determine whether the project has sufficient funding available to support bioinformatics collaboration.&lt;/p></description></item><item><title>Journal Club Papers</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/blog/journal-club/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/blog/journal-club/</guid><description>&lt;h2 id="benchmarking-papers">Benchmarking Papers&lt;/h2>
&lt;h2 id="reproducibility">Reproducibility&lt;/h2>
&lt;h2 id="new-tools">New Tools&lt;/h2></description></item><item><title>Lab Coding Guidelines</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/lab-coding-guidelines/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/lab-coding-guidelines/</guid><description>&lt;p>This is a summary of the 2018-10-10 lab meeting where we discussed coding practices. The &lt;a href="https://hackmd.io/K4ARCohDQN2YTj6Reb3Vtw" target="_blank" rel="noopener">full notes&lt;/a> are available online.&lt;/p>
&lt;h2 id="desired-qualities">Desired Qualities:&lt;/h2>
&lt;ul>
&lt;li>completeness
&lt;ul>
&lt;li>code for all the figures and analyses&lt;/li>
&lt;li>external dependencies documented&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>readable
&lt;ul>
&lt;li>uses good standards for code style&lt;/li>
&lt;li>comments help guide navigation to different parts&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>(re)usable
&lt;ul>
&lt;li>description of how to run everything, few changes needed to run it all&lt;/li>
&lt;li>examples for functions&lt;/li>
&lt;li>functions written to be flexible (e.g. less dependence on &amp;ldquo;magic numbers&amp;rdquo; and hard-coded parameter values)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="practices">Practices:&lt;/h2>
&lt;ul>
&lt;li>linter (for style)&lt;/li>
&lt;li>tests (to check functions, could also provide simple examples)&lt;/li>
&lt;li>pair programming checks for readability&lt;/li>
&lt;li>documentation&lt;/li>
&lt;li>refactoring core code into reusable packages&lt;/li>
&lt;li>containers&lt;/li>
&lt;/ul>
&lt;h2 id="action-items">Action Items:&lt;/h2>
&lt;ul>
&lt;li>training (and organizing it)&lt;/li>
&lt;li>toolchain (linter, tests, development tools)&lt;/li>
&lt;li>workflow and organization&lt;/li>
&lt;li>regular practices&lt;/li>
&lt;/ul></description></item><item><title>Parallelizing Code in R</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/parallelization-r/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/parallelization-r/</guid><description>&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The basic idea of parallelization is the running of computational tasks simultaneously, as opposed to sequentially (or &amp;ldquo;in parallel&amp;rdquo; as opposed to &amp;ldquo;in sequence&amp;rdquo;). To do this, the computer needs to be able to split code into pieces that can run independently and then be joined back together as if they had been run sequentially. The parts of the computer that run the pieces of code are processing units and are typically called &amp;ldquo;cores&amp;rdquo;.&lt;/p>
&lt;p>The &lt;code>doParallel&lt;/code> package is (as far as I&amp;rsquo;m currently aware) the most platform-general and robust parallel package available in R. There is more functionality for Unix-alikes in the &lt;code>parallel&lt;/code> package (see link below), but that doesn&amp;rsquo;t transfer to Windows machines.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">library(doParallel)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="cores-and-clusters">Cores and Clusters&lt;/h2>
&lt;p>At the outset, it&amp;rsquo;s important to know how many cores you have available, which the &lt;code>detectCores()&lt;/code> function returns. However, the value returned includes &amp;ldquo;hyperthreaded cores&amp;rdquo; (aka &amp;ldquo;logical cores&amp;rdquo;), which include further computational divvyings of the physical cores.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">detectCores()
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">detectCores(logical = FALSE)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Although hyperthreaded cores are available, they often do not show speed gains in R. However, as with everything parallel in R, it&amp;rsquo;s best to actually test that on the problem you&amp;rsquo;re working on, as there will be gains in certain situations.&lt;/p>
&lt;p>At the user-interface level, we only interact at a single point, yet the code needs to access multiple cores at once. To achieve this, we have the concept of a &amp;ldquo;cluster&amp;rdquo;, which represents a parallel set of copies of R running across multiple cores (including across physical sockets). We create a cluster using the &lt;code>makeCluster&lt;/code> function and register the backend using the &lt;code>registerDoParallel&lt;/code> function:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">ncores &amp;lt;- detectCores(logical = FALSE)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cl &amp;lt;- makeCluster(floor(0.75 * ncores))
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">registerDoParallel(cl)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stopCluster(cl)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, I&amp;rsquo;ve stopped the cluster explicitly using &lt;code>stopCluster&lt;/code>, which frees up the computational resources. The cluster will be automatically stopped when you end your R instance, but it&amp;rsquo;s a good habitat to be in to stop any clusters you make (even if you don&amp;rsquo;t register the backend).&lt;/p>
&lt;h2 id="foreach-and-dopar">foreach and %dopar%&lt;/h2>
&lt;p>Parallelization in &lt;code>doParallel&lt;/code> happens via the combination of the &lt;code>foreach&lt;/code> and &lt;code>%dopar%&lt;/code> operators in a fashion similar to &lt;code>for&lt;/code> loops. Rather than &lt;code>for(variable in values) {expression}&lt;/code>, we have &lt;code>foreach(variable = values, options) %dopar% {expression}&lt;/code>. Thus, the basic code block of a &lt;code>foreach&lt;/code> parallel loop is&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="n">cl&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">makeCluster&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">floor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.75&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">ncores&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">registerDoParallel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cl&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">foreach&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">variable&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">values&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">options&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span>&lt;span class="n">dopar&lt;/span>&lt;span class="o">%&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expression&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">stopCluster&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cl&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that there can be multiple variable arguments (&lt;em>e.g.&lt;/em>, &lt;code>foreach(i = 1:10, j = 11:20) %dopar% {}&lt;/code>), although no variables are recycled, so the smallest number of values across variables is used.&lt;/p>
&lt;p>Each of the instances of a &lt;code>foreach&lt;/code> is referred to as a &amp;ldquo;task&amp;rdquo; (a key word, especially in terms of error checking, see link below).&lt;/p>
&lt;p>An important distinction between &lt;code>foreach&lt;/code> and &lt;code>for&lt;/code> is that &lt;code>foreach&lt;/code> returns a value (by default, a list), whereas &lt;code>for&lt;/code> causes side effects. Compare the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">cl &amp;lt;- makeCluster(floor(0.75 * ncores))
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">registerDoParallel(cl)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">out &amp;lt;- foreach(i = 1:10) %dopar% {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> i + rnorm(1, i, i)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stopCluster(cl)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>vs.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">out &amp;lt;- vector(&amp;#34;list&amp;#34;, 10)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">for(i in 1:10){
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> out[[i]] &amp;lt;- i + rnorm(1, i, i)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>foreach&lt;/code> creates &lt;code>out&lt;/code>; &lt;code>for&lt;/code> modifies it.&lt;/p>
&lt;p>There are a handful of option arguments in &lt;code>foreach&lt;/code> that are really critical for anything beyond trivial computation:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.packages&lt;/code> passes the libraries down to the cluster of Rs. If you don&amp;rsquo;t include package names and you use a package-derived function, it won&amp;rsquo;t work&lt;/li>
&lt;li>&lt;code>.combine&lt;/code> dictates how the output is combined, defaulting to a list, but allowing lots of flexibility including mathematical operations. If needed, the &lt;code>.init&lt;/code> option allows you to initialize the value (for example with 0 or 1).&lt;/li>
&lt;li>&lt;code>.inorder&lt;/code> sets whether the combination happens based on the order of inputs. If the order isn&amp;rsquo;t important, setting this to &lt;code>FALSE&lt;/code> can give performance gains.&lt;/li>
&lt;li>&lt;code>.errorhandling&lt;/code> determines what happens when a task fails (see link below for using &lt;code>tryCatch&lt;/code> within tasks).&lt;/li>
&lt;/ul>
&lt;p>In addition to &lt;code>%dopar%&lt;/code>, there is a sequential operator for use with &lt;code>foreach&lt;/code>, and it is simply &lt;code>%do%&lt;/code>. Replacing &lt;code>%dopar%&lt;/code> with &lt;code>%do%&lt;/code> will cause the code to run in-order, as if you did not initiate a cluster. Similarly, you can run &lt;code>foreach&lt;/code> and &lt;code>%dopar%&lt;/code> without having made a cluster, but the code will be run sequentially.&lt;/p>
&lt;h2 id="nesting-foreach-loops">Nesting foreach loops&lt;/h2>
&lt;p>Nested loops can often be really powerful for computation. &lt;code>doParallel&lt;/code> has a special operator that combines two &lt;code>foreach&lt;/code> objects in a nested manner: &lt;code>%:%&lt;/code>. This operator causes the outer most &lt;code>foreach&lt;/code> to be evaluated over its variables&amp;rsquo; values, which are then passed down into the next innermost &lt;code>foreach&lt;/code>. That &lt;code>foreach&lt;/code> iterates over its variables&amp;rsquo; values for each value of the outer &lt;code>foreach&lt;/code> variables.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">cl &amp;lt;- makeCluster(floor(0.75 * ncores))
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">registerDoParallel(cl)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">out &amp;lt;- foreach(i = 1:10) %:%
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> foreach(j = 1:100) %dopar% {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> i * j^3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">out
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stopCluster(cl)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>.combine&lt;/code> option is really important to pay attention to with nested &lt;code>foreach&lt;/code> loops, as it will allow you to flexibly structure the data. The above version produces a list (length 10) of lists (length 100).&lt;/p>
&lt;h2 id="seeds-and-rngs">Seeds and RNGs&lt;/h2>
&lt;p>One of the downfalls of &lt;code>foreach&lt;/code> and &lt;code>%dopar%&lt;/code> is that the parallel runs aren&amp;rsquo;t reproducible in a simple way. There are ways to code seed setting up, but it&amp;rsquo;s a little obtuse. Thankfully, the &lt;code>doRNG&lt;/code> package has you taken care of. There are a few ways to code up a reproducible parallel loop.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">library(doRNG)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cl &amp;lt;- makeCluster(floor(0.75 * ncores))
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">registerDoParallel(cl)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># 1. .options.RNG
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">out1 &amp;lt;- foreach(i = 1:10, .options.RNG = 1234) %dorng% {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> rnorm(1, i, i^2)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># 2. set.seed
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">set.seed(1234)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">out2 &amp;lt;- foreach(i = 1:10) %dorng% {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> rnorm(1, i, i^2)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># 3. registerDoRNG (note that this doesn&amp;#39;t replace the registerDoParallel!)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">registerDoRNG(1234)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">out3 &amp;lt;- foreach(i = 1:10) %dorng% {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> rnorm(1, i, i^2)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stopCluster(cl)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">identical(out1, out2)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">identical(out1, out3)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="speed-gains">Speed Gains&lt;/h2>
&lt;p>The degree to which your code speeds up when you parallelize it depends on a few factors: how many cores you have, whether the code can take advantage of hyperthreading, the run time of the code itself, and your ease of coding in parallel. There is definitely some computational and time overhead involved in setting up a cluster, distributing the work, and combining the results. Short tasks (seconds to a half-minute), therefore, are usually faster to run in sequence. Once tasks start creeping up to the half-minute mark on a dual-core machine (or the less time on a more-core machine), the parallel runs will start to be faster. As the runtime of the computation increases, the relative gain by parallelizing increases, although it never gets to a fully fractional decrease of time (&lt;em>i.e.&lt;/em>, 6 cores won&amp;rsquo;t ever get you to 1/6 the runtime&amp;hellip;probably more like 1/4 - 1/5) due to overhead. The other thing to keep in mind with parallelization is that there is often some additional coding time involved, and there can be issues that require an additional level of troubleshooting that quickly eliminates the time gains.&lt;/p>
&lt;p>See references below for speed tests.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>
&lt;/li>
&lt;li>
&lt;/li>
&lt;li>
&lt;/li>
&lt;li>
&lt;/li>
&lt;li>
&lt;/li>
&lt;li>
&lt;/li>
&lt;li>
&lt;/li>
&lt;li>
&lt;/li>
&lt;/ul></description></item><item><title>Python - Package Documentation</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/python-package-documentation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/python-package-documentation/</guid><description>&lt;h2 id="add-docs">Add docs&lt;/h2>
&lt;h3 id="install-sphinx-and-the-markdown-parser">Install sphinx and the markdown parser&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">pip install sphinx myst-parser
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="make-a-docs-directory-and-run-the-quick-start">Make a docs directory and run the quick-start&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">mkdir docs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> docs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sphinx-quickstart
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="add-markdown-and-autodoc-extensions-to-confpy">Add markdown and autodoc extensions to &lt;code>conf.py&lt;/code>&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">extensions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;myst_parser&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;sphinx.ext.autodoc&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="make-docs-files-and-edit-indexrst">Make docs files and edit index.rst&lt;/h3>
&lt;ul>
&lt;li>Edit &lt;code>index.rst&lt;/code> to include the information you&lt;/li>
&lt;li>Add markdown files for each separate page of docs&lt;/li>
&lt;li>In &lt;code>index.rst&lt;/code> add the names of the markdown files (without extensions) to the &lt;code>toctree&lt;/code> block. E.g., if we want to include an the docs in &lt;code>installation.md&lt;/code> and &lt;code>getting-started.md&lt;/code>:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">.. toctree::
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> :maxdepth: 2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> :caption: Contents:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> installation
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> getting-started
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="setup-automatic-function-documentation">Setup automatic function documentation&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">sphinx-apidoc -f -o &lt;span class="nb">source&lt;/span> ../&amp;lt;package-name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="change-the-theme">Change the theme&lt;/h3>
&lt;ul>
&lt;li>Pick a theme (we currently use either spinx-rtd-theme or furo)&lt;/li>
&lt;li>Install it&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">pip install sphinx_rtd_theme
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Change the theme value in in &lt;code>conf.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">html_theme&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;sphinx_rtd_theme&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>If using the &lt;code>sphinx_rtd_theme&lt;/code> also add it to &lt;code>extensions&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">extensions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;myst_parser&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;sphinx.ext.autodoc&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;sphinx_rtd_theme&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="build-the-docs">Build the docs&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">make html
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="build-docs-automatically-on-readthedocs">Build docs automatically on readthedocs&lt;/h2>
&lt;p>Your project should be in an online repository&lt;/p>
&lt;h3 id="add-a-docs-requirementstxt-file">Add a docs requirements.txt file&lt;/h3>
&lt;p>In the docs directory add a &lt;code>requirement.txt&lt;/code> file that includes the extra packages required for building the docs.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">myst_parser
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sphinx_rtd_theme
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="add-readthedocsyaml">Add .readthedocs.yaml&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">build&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">os&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;ubuntu-22.04&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">tools&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">python&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;3.10&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">python&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">install&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">method&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">pip&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">.&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">requirements&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">docs/requirements.txt&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">sphinx&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">configuration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">docs/conf.py&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="connect-your-githubgitlab-account-to-readthedocs">Connect your GitHub/GitLab account to readthedocs&lt;/h3>
&lt;ul>
&lt;li>Go to
&lt;/li>
&lt;li>Click &amp;lsquo;Sign up&amp;rsquo;&lt;/li>
&lt;li>Choose &amp;lsquo;Read the Docs Community&amp;rsquo;&lt;/li>
&lt;li>Click &amp;lsquo;Sign up with GitHub&amp;rsquo; (or GitLab)&lt;/li>
&lt;li>Follow the instructions&lt;/li>
&lt;/ul>
&lt;h3 id="connect-your-project">Connect your project&lt;/h3>
&lt;ul>
&lt;li>Go to
&lt;/li>
&lt;li>Click &amp;lsquo;Import a Project&amp;rsquo;&lt;/li>
&lt;li>If the project is listed select it&lt;/li>
&lt;li>If it is not listed click on &amp;lsquo;Import Manually&amp;rsquo; and provide the requested information&lt;/li>
&lt;/ul>
&lt;h3 id="enable-builds-for-prs">Enable builds for PRs&lt;/h3>
&lt;p>If you want to check the doc builds from your PRs enable this by:&lt;/p>
&lt;ol>
&lt;li>Go to the project dashboard on readthedocs&lt;/li>
&lt;li>Select &amp;lsquo;Admin&amp;rsquo;&lt;/li>
&lt;li>Select &amp;lsquo;Advanced Settings&amp;rsquo;&lt;/li>
&lt;li>Click &amp;lsquo;Build pull requests for this project&amp;rsquo;&lt;/li>
&lt;/ol></description></item><item><title>R Resources</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/r-resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/r-resources/</guid><description>&lt;p>R is a powerful tool that allows you to do a lot of things such as doing simple arithmetic calculations, organizing and analyzing your data, and even developing your own software. There are several, &lt;strong>FREE&lt;/strong> resources that you can use to learn R. Below is a &lt;em>non-exhaustive&lt;/em> list of tutorials, books, and videos that you can use (most of which are useful for data analysis), whether you&amp;rsquo;re just starting out or are more advanced in programming.&lt;/p>
&lt;h2 id="r-at-an-introductory-level">R at an Introductory Level&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://swirlstats.com/students.html" target="_blank" rel="noopener">swirlR&lt;/a> is an R package that allows you to learn R in the console at your own pace&lt;/li>
&lt;li>&lt;a href="https://tinystats.github.io/teacups-giraffes-and-statistics/index.html" target="_blank" rel="noopener">Teacups, Giraffes, and Statistics&lt;/a> is an interactive website that contains modules useful for learning statistics and R&lt;/li>
&lt;li>&lt;a href="https://rforcats.net/" target="_blank" rel="noopener">R for cats&lt;/a> is an introductory guide to the use of R with cat photos (this is a bonus!)&lt;/li>
&lt;li>&lt;a href="https://paulvanderlaken.files.wordpress.com/2017/08/r_in_a_nutshell.pdf" target="_blank" rel="noopener">R in a Nutshell&lt;/a> is a book that gives you a concise overview of the different things you can do in R&lt;/li>
&lt;li>&lt;a href="https://www.infoworld.com/article/3411819/do-more-with-r-video-tutorials.html" target="_blank" rel="noopener">Do More with R&lt;/a> is a website listing video tutorials on specific topics in R (most videos are said to be &amp;lt; 10 minutes in length)&lt;/li>
&lt;li>&lt;a href="https://datacarpentry.org/semester-biology/readings/R-intro/" target="_blank" rel="noopener">Data Carpentry for Biologists&lt;/a> is derived from the semester-long course taught By Dr. Ethan White that covers basic functions/use of R&lt;/li>
&lt;/ul>
&lt;h2 id="intermediate-or-advanced-use-of-r">Intermediate or Advanced use of R&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://rstats.wtf/" target="_blank" rel="noopener">What They Forgot to Teach You about R&lt;/a> is a set of tips on doing effective, reproducible data analysis in R (and less about actual programming)&lt;/li>
&lt;li>&lt;a href="https://happygitwithr.com/" target="_blank" rel="noopener">Happy Git and GitHub for the UseR&lt;/a> is an instructional guide to using Git, GitHub, and R, which are all tools we use in the lab&lt;/li>
&lt;li>&lt;a href="https://mathstat.slu.edu/~speegle/_book/RData.html" target="_blank" rel="noopener">Foundations of Statistics with R&lt;/a> is a course book for learning probability and statistics using R&lt;/li>
&lt;li>&lt;a href="https://csgillespie.github.io/efficientR/introduction.html" target="_blank" rel="noopener">Efficient R Programming&lt;/a> is another book that would help you increase your algorithmic and programming efficiency when using R&lt;/li>
&lt;li>&lt;a href="https://adv-r.hadley.nz/preface.html" target="_blank" rel="noopener">Advanced R&lt;/a> is the second edition of a book that teaches you more advanced programming skills in R (it makes use of a new package called &lt;a href="https://rlang.r-lib.org/" target="_blank" rel="noopener">rlang&lt;/a>, which is an interface to low-level data structures and operations&lt;/li>
&lt;li>&lt;a href="https://ms.mcmaster.ca/~bolker/emdbook/book.pdf" target="_blank" rel="noopener">Ecological Models and Data in R&lt;/a> is a book about building models implemented in a frequentist or Bayesian framework to answer ecological questions&lt;/li>
&lt;/ul>
&lt;h2 id="getting-help">Getting Help&lt;/h2>
&lt;p>There are times when your code might not seem to work. Apart from the brilliant lab members who you can easily reach out to through Slack for help, you can post questions/concerns in different online communities such as:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://community.rstudio.com/" target="_blank" rel="noopener">RStudio Community&lt;/a> for R-specific questions&lt;/li>
&lt;li>&lt;a href="https://stackoverflow.com/" target="_blank" rel="noopener">Stack Overflow&lt;/a> for programming questions&lt;/li>
&lt;/ul></description></item><item><title>Rodent images</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/miscellaneous/rodent-images/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/miscellaneous/rodent-images/</guid><description>&lt;p>Rodent silhouette images for use in publications, etc.&lt;/p>
&lt;p>&lt;a href="https://github.com/weecology/rodent-silhouettes" target="_blank" rel="noopener">These rodent silhouettes&lt;/a> were commissioned by the lab and paid for, so we can use them freely. The artist&amp;rsquo;s name, for credits, is Marcela Diaz.&lt;/p></description></item><item><title>Using Git From RStudio</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/rstudio-git-integration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/rstudio-git-integration/</guid><description>&lt;p>Just a brief introduction on how to easily set up a Git/GitHub repo in RStudio:&lt;/p>
&lt;ul>
&lt;li>Make sure Git is set up on your computer and in R (ask Ethan for help on this one)&lt;/li>
&lt;li>Sign into your GitHub account&lt;/li>
&lt;li>Click on the plus sign in the top right-hand corner (near your profile picture) and select &amp;ldquo;New repository&amp;rdquo;. You can choose to make your new repo through your account or the Weecology one on the next page.&lt;/li>
&lt;li>Name your repository, add a quick description if desired, select Public or Private (most often Public) and &lt;em>be sure to check the box to initialize a README document&lt;/em>. Click &lt;em>Create Repository.&lt;/em>&lt;/li>
&lt;li>You&amp;rsquo;ll now be on the page for your new repo. Find the green icon &lt;strong>CODE&lt;/strong> that has either an https address or something that looks like an email address (SSH). Make sure that the box to the left of text box says HTTPS rather than SSH. Then click the clipboard icon to the right of the text box. This will copy that address to your clipboard.&lt;/li>
&lt;li>Now you&amp;rsquo;re done with GitHub. Open up RStudio. Go to &lt;em>File&lt;/em> and select &lt;em>New Project&lt;/em>. If Git is properly set up in RStudio, you should have an option called &lt;em>Version Control&lt;/em>. Click on that option.&lt;/li>
&lt;li>Select the &lt;em>Git&lt;/em> option from the next menu.&lt;/li>
&lt;li>Paste the HTTPS address into the &lt;em>Repository URL&lt;/em> box. It will auto-fill the &lt;em>Project directory name&lt;/em>. Make sure the project is being created in the appropriate folder.&lt;/li>
&lt;li>Click &lt;em>Create Project&lt;/em> and voila!&lt;/li>
&lt;li>Now you should see a &lt;em>Git&lt;/em> tab next to the &lt;em>Environment&lt;/em> and &lt;em>History&lt;/em> tabs. From there, you can make commits, push (green up arrow), and pull (blue down arrow).&lt;/li>
&lt;/ul></description></item><item><title>Search Results</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/search/</guid><description/></item><item><title>Computer Security &amp; Privacy</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/computer-security-privacy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/computer-security-privacy/</guid><description>&lt;h2 id="password-managers">Password Managers&lt;/h2>
&lt;p>A password manager keeps track of the various passwords you use for different websites. (And the reason to use a different password for each account, is so that if your password at one place is compromised, it does not affect your other accounts.)&lt;/p>
&lt;ul>
&lt;li>Commonly recommended password managers are &lt;a href="https://www.lastpass.com" target="_blank" rel="noopener">Lastpass&lt;/a> and &lt;a href="https://1password.com" target="_blank" rel="noopener">1password&lt;/a>.&lt;/li>
&lt;li>Lastpass has a free option, but will only sync passwords across similar device-types (e.g. between computers, or between smartphones), but not between computer and smartphone. Paid plans for Lastpass start at $2/month.&lt;/li>
&lt;li>1password seems to have a nicer interface, but plans start at $2.99/month.&lt;/li>
&lt;li>The common usage for password managers is as browser plug-ins that can remember passwords, generate new random passwords, and autofill.&lt;/li>
&lt;li>Both Lastpass and 1password have secure notes, which allow you to keep track of answers to security questions, in case you want to generate random responses rather than use the middle school you went to, which anyone can look up.&lt;/li>
&lt;li>Apple&amp;rsquo;s iCloud Keychain is another option if you have all Apple devices. Here&amp;rsquo;s a &lt;a href="https://www.macworld.com/article/3060630/ios/why-not-pick-keychain-instead-of-1password-or-lastpass.html" target="_blank" rel="noopener">discussion&lt;/a> on the differences with third-party password managers.&lt;/li>
&lt;li>Since your email usually allows you to reset passwords, it can be preferable to NOT put your email password into your password manager, and instead to secure your email with another strong passphrase.&lt;/li>
&lt;/ul>
&lt;h2 id="passphrases">Passphrases&lt;/h2>
&lt;p>Because password managers are secured by a single master passphrase (that unlocks all your passwords), it is recommended that you use a strong passphrase.&lt;/p>
&lt;ul>
&lt;li>One method is to string together random words, possibly with capitalization and special characters added.&lt;/li>
&lt;li>Rather than use a website for this (who knows if it is truly random or recording the output it gives you), you can use &lt;a href="https://www.eff.org/dice" target="_blank" rel="noopener">dice and a word list&lt;/a>.&lt;/li>
&lt;li>While trying to memorize the passphrase, you might consider having a written copy that you keep somewhere safe.&lt;/li>
&lt;/ul>
&lt;h2 id="two-factor-authentication">Two-Factor Authentication&lt;/h2>
&lt;p>Two-factor authentication (commonly 2FA or MFA for multi-factor) means identifying yourself using components from at least two different categories:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>something you know (e.g. a password)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>something you have (e.g. a key or phone)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>something you are (e.g. fingerprints, biometrics)
Thus, if someone else has your password, they still can&amp;rsquo;t access your account without e.g. your phone, too.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Many large sites (e.g. banks, email) with security concerns have 2FA as an option, but it might need to be turned on. For example, here are the instructions for &lt;a href="https://www.google.com/landing/2step/" target="_blank" rel="noopener">gmail&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Common implementations are to send a code by email or text when you log in.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Some places also support apps or devices that generate rolling codes (e.g. &lt;a href="https://support.google.com/accounts/answer/1066447?co=GENIE.Platform%3DAndroid&amp;amp;hl=en" target="_blank" rel="noopener">Google Authenticator&lt;/a>). These generate rolling codes that rotate regularly, for example, every minute. When synced up with a website during 2FA setup, you can also use the rolling code to authenticate.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hardware tokens are also possible, for example &lt;a href="https://www.yubico.com/products/yubikey-hardware/" target="_blank" rel="noopener">Yubikeys&lt;/a>. You will want to check compatibility with services. For instance, the basic U2F key will not work as 2FA for Lastpass, and you also need a paid Lastpass plan.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="misc">Misc.&lt;/h2>
&lt;ul>
&lt;li>EFF has a &lt;a href="https://www.eff.org/https-everywhere" target="_blank" rel="noopener">browser extension&lt;/a> to use encrypted HTTPS when possible.&lt;/li>
&lt;li>EFF also has a &lt;a href="https://www.eff.org/privacybadger" target="_blank" rel="noopener">browser extension&lt;/a> to block ads from tracking you across websites.&lt;/li>
&lt;li>Don&amp;rsquo;t plug in unknown USB devices to your computer or your USB devices into unknown ports. (&lt;a href="https://www.reuters.com/article/us-nuclearpower-cyber-germany/german-nuclear-plant-infected-with-computer-viruses-operator-says-idUSKCN0XN2OS" target="_blank" rel="noopener">e.g. &amp;#x1f631;&lt;/a>:&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>As an example, Hypponen said he had recently spoken to a European aircraft maker that said it cleans the cockpits of its planes every week of malware designed for Android phones. The malware spread to the planes only because factory employees were charging their phones with the USB port in the cockpit.&lt;/p>
&lt;p>Because the plane runs a different operating system, nothing would befall it. But it would pass the virus on to other devices that plugged into the charger.&lt;/p>&lt;/blockquote>
&lt;ul>
&lt;li>You can use incognito / private mode when browsing to bypass some paywalls (e.g. NYT&amp;rsquo;s 10-article/month limit)&lt;/li>
&lt;li>&lt;a href="https://objective-see.com/products/oversight.html" target="_blank" rel="noopener">OverSight&lt;/a> comes recommended by some folks as a way to notify about microphone and camera usage on Macs.&lt;/li>
&lt;/ul></description></item><item><title>Software Testing in R</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/software-testing-r/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/software-testing-r/</guid><description>&lt;p>&lt;a href="https://github.com/weecology/rtestpackage" target="_blank" rel="noopener">https://github.com/weecology/rtestpackage&lt;/a>&lt;/p>
&lt;p>This repo contains a description of how to setup the test environment for R.&lt;/p>
&lt;p>There are additional references from where the information was derived.&lt;/p></description></item><item><title>Statistics for Software Use</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/statistics-for-software-downloads/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/statistics-for-software-downloads/</guid><description>&lt;p>It can be helpful to know how frequently your software is being downloaded to assess it&amp;rsquo;s use and report on its impact. Below are instructions for how to do this for different languages. Keep in mind that downloads can be influenced by automated testing systems if they install the software from the central repository.&lt;/p>
&lt;h2 id="r">R&lt;/h2>
&lt;p>These instructions get downloads from the cloud CRAN mirror. This is a minimum estimate of total downloads.&lt;/p>
&lt;ul>
&lt;li>Install the &lt;a href="https://github.com/r-hub/cranlogs" target="_blank" rel="noopener">&lt;code>cranlogs&lt;/code> package&lt;/a> &lt;code>install.packages(&amp;quot;cranlogs&amp;quot;)&lt;/code>&lt;/li>
&lt;li>Run the &lt;code>cran_downloads&lt;/code> function with your package name and date ranges if desired&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">downloads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cranlogs&lt;/span>&lt;span class="o">::&lt;/span>&lt;span class="nf">cran_downloads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">packages&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;portalr&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">from&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;2020-02-26&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;2020-08-16&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">total_downloads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">downloads&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="python">Python&lt;/h2>
&lt;p>Python packages are often distributed using both PyPI and conda so you might need to get download statistics for both.&lt;/p>
&lt;h3 id="pypi">PyPI&lt;/h3>
&lt;h4 id="using-the-pypi-stats-website">Using the &lt;a href="https://pypistats.org/" target="_blank" rel="noopener">PyPI Stats website&lt;/a>&lt;/h4>
&lt;ul>
&lt;li>PyPI Stats is easy and provides the last 6 months of data.&lt;/li>
&lt;li>E.g., &lt;a href="https://pypistats.org/packages/deepforest" target="_blank" rel="noopener">https://pypistats.org/packages/deepforest&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="using-google-bigquery">Using Google BigQuery&lt;/h4>
&lt;ul>
&lt;li>Setup a Google BigQuery account&lt;/li>
&lt;li>Run a version of this query where with additional modifications as necessary&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-Python" data-lang="Python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">SELECT&lt;/span> &lt;span class="n">COUNT&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">downloads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">FROM&lt;/span> &lt;span class="err">`&lt;/span>&lt;span class="n">the&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">psf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pypi&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downloads2020&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="err">`&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">WHERE&lt;/span> &lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">project&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;retriever&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="conda">Conda&lt;/h3>
&lt;ul>
&lt;li>Install the &lt;a href="https://www.anaconda.com/blog/get-python-package-download-statistics-with-condastats" target="_blank" rel="noopener">&lt;code>condastats&lt;/code> package&lt;/a> &lt;code>conda install -c conda-forge condastats&lt;/code>&lt;/li>
&lt;li>From the command line run a version of the following command&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">condastats overall retriever --start_month 2020-01 --end_month 2020-08
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>SSH</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/ssh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/ssh/</guid><description>&lt;h2 id="what-is-ssh">What is SSH?&lt;/h2>
&lt;p>SSH (short for &amp;ldquo;secure shell&amp;rdquo;) is a computer protocol for encrypting access to computers over a network. Typical use cases are:&lt;/p>
&lt;ul>
&lt;li>accessing HiPerGator and Serenity servers from a laptop or desktop machine, while on the UF network&lt;/li>
&lt;li>cloning, pulling, or pushing to GitHub repositories&lt;/li>
&lt;/ul>
&lt;h2 id="what-software-do-i-need">What software do I need?&lt;/h2>
&lt;p>SSH should be pre-installed with Mac OS, Windows 10, and Linux.
On other versions of Windows, you might want to download
which also contains some other useful tools.&lt;/p>
&lt;h2 id="what-is-an-ssh-key">What is an SSH key?&lt;/h2>
&lt;p>Access to servers through SSH involves authenticating yourself with a username and password (the same as if you were logging in directly to the machine).&lt;/p>
&lt;p>Alternatively, you can generate a digital &amp;ldquo;key&amp;rdquo; that provides access to the keyholder. Typically, you generate an SSH key for your computer, and then set up the appropriate file on the server(s) you wish to access. Then, instead of authenticating yourself with your username and password for the server, you instead use the SSH key, which is verified by the paired file previously copied to the server. (You can also provide a passphrase in order to use your SSH key, but as it&amp;rsquo;s stored on your laptop or desktop and requires you to be logged in, this security step is optional - opinions vary on this.)&lt;/p>
&lt;h2 id="setup-instructions-github">Setup Instructions (GitHub)&lt;/h2>
&lt;p>GitHub has fairly detailed
on creating a new SSH key;&lt;/p>
&lt;p>and further
to enable it for use on Github.&lt;/p>
&lt;h2 id="setup-instructions-serenity--hipergator--generic">Setup Instructions (Serenity / HiPerGator / generic)&lt;/h2>
&lt;p>To setup an SSH key for use on Serenity, we assume you have gone through the steps described in the instructions for GitHub to create an SSH key. This creates both the key itself (usually located in &lt;code>~/.ssh/id_rsa&lt;/code>), and a paired file that verifies that the key is correct (usually located in &lt;code>~/.ssh/id_rsa.pub&lt;/code>).&lt;/p>
&lt;p>&lt;strong>If you are setting up an SSH key on HiPerGator, note that you probably already have one in the default location, which is used for communicating between different HiPerGator nodes. You may only need to follow the
to enable its usage for github, as well.&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>Log in to the server you wish to use the SSH key on.&lt;/li>
&lt;li>Edit the &lt;code>~/.ssh/authorized_keys&lt;/code> file &lt;em>on the server&lt;/em>. It may already have contents, in which case, go to a new, blank line.&lt;/li>
&lt;li>Copy over the contents of the &lt;code>~/.ssh/id_rsa.pub&lt;/code> from your local computer. It will look something like:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">ssh-rsa *************long-string of random characters************* &amp;lt;email address&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>(If you see something like: &lt;code>-----BEGIN RSA PRIVATE KEY-----&lt;/code>, you are using the wrong file!!)&lt;/strong>&lt;/p>
&lt;ol start="4">
&lt;li>Save the modified &lt;code>~/.ssh/authorized_keys&lt;/code> file on the server.&lt;/li>
&lt;li>Try to connect using ssh. In the command line on your local computer:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssh &amp;lt;username&amp;gt;@&amp;lt;server&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should authenticate without having to enter your password.&lt;/p></description></item><item><title>Who We Are</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/collaborating-with-us/how-to-work-with-us/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/collaborating-with-us/how-to-work-with-us/</guid><description>&lt;h2 id="who-we-are">Who We Are&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://directory.ufhealth.org/brant-jason/" target="_blank" rel="noopener">Dr. Jason O. Brant, Unit Leader&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://directory.ufhealth.org/kates-heather/" target="_blank" rel="noopener">Dr. Heather R. Kates, Bioinformatics Analyst III&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://directory.ufhealth.org/shirlekar-kalyanee" target="_blank" rel="noopener">Kalyanee Shirlekar, MS, Bioinformatics Analyst III&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="why-uf-health-cancer-center-bcb-sr">Why UF Health Cancer Center BCB-SR?&lt;/h2>
&lt;p>Each of us at BCB-SR Bioinformatics is driven by the mission to bridge the gap between data and discovery. We are all trained as biologists as well as bioinformaticians, and take a biology-first view of bioinformatics analysis.&lt;/p>
&lt;h2 id="careers">Careers&lt;/h2>
&lt;p>Want to work as a bioformatician at UF? We&amp;rsquo;d love to hear from you to support your career development. You can search for jobs at &lt;a href="https://explore.jobs.ufl.edu/en-us/listing/" target="_blank" rel="noopener">Careers at UF&lt;/a> using broad search terms including &amp;ldquo;data&amp;rdquo; and &amp;ldquo;analyst&amp;rdquo;, as not all programs realize that what they are looking for is a &lt;em>bioinformatician&lt;/em>!&lt;/p></description></item><item><title>Workflow Tools - snakemake &amp; targets</title><link>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/workflow-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ufhcc-bcbsr.github.io/wiki/docs/computers-and-programming/workflow-management/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Workflow tools let you automate the running and rerunning of code with multiple steps.
For example we use it for managing our image processing workflow for our &lt;a href="https://everglades.weecology.org/" target="_blank" rel="noopener">Everglades research&lt;/a>.&lt;/p>
&lt;h2 id="python---snakemake">Python - snakemake&lt;/h2>
&lt;h3 id="getting-started">Getting Started&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html" target="_blank" rel="noopener">Official snakemake tutorial&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://farm.cse.ucdavis.edu/~ctbrown/2023-snakemake-book-draft/" target="_blank" rel="noopener">C. Titus Brown&amp;rsquo;s draft book on using snakemake for bioinformatics&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="handling-complex-inputs-with-input-functions">Handling complex inputs with input functions&lt;/h3>
&lt;p>In our workflows with deal with complex input-output structures, like having early phases of the pipeline work on one flight (file) at a time and later phases work on all of the files from a given site and year as a group.&lt;/p>
&lt;p>This can be accomplished by defining custom input functions.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.embl.org/groups/bioinformatics-rome/blog/2022/10/guest-post-snakemake-input-functions-by-tim-booth/" target="_blank" rel="noopener">Introduction to Input Functions&lt;/a>&lt;/li>
&lt;li>See our &lt;a href="https://github.com/weecology/EvergladesTools/blob/main/Zooniverse/Snakefile" target="_blank" rel="noopener">Everglades workflow Snakefile&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="testing-snakemake-with-partial-wildcards">Testing snakemake with partial wildcards&lt;/h3>
&lt;p>When testing big workflow it is often useful to run the workflow on a subset of the data.
For example our Everglades workflow runs on all years, sites, and flight at once, but we might want to test a site year-site combination when making a change.
To prepare to do this replace your Wildcards object with the component lists for the main workflow. E.g.,&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">ORTHOMOSAICS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">glob_wildcards&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/&lt;/span>&lt;span class="si">{year}&lt;/span>&lt;span class="s2">/&lt;/span>&lt;span class="si">{site}&lt;/span>&lt;span class="s2">/&lt;/span>&lt;span class="si">{flight}&lt;/span>&lt;span class="s2">.tif&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">FLIGHTS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ORTHOMOSAICS&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">flight&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">SITES&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ORTHOMOSAICS&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">site&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">YEARS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ORTHOMOSAICS&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">year&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The components are just lists, so you can then replace them with whatever pieces of the full workflow you want to test. E.g.,:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">ORTHOMOSAICS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">glob_wildcards&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/&lt;/span>&lt;span class="si">{year}&lt;/span>&lt;span class="s2">/&lt;/span>&lt;span class="si">{site}&lt;/span>&lt;span class="s2">/&lt;/span>&lt;span class="si">{flight}&lt;/span>&lt;span class="s2">.tif&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">TEST&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">glob_wildcards&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/blue/ewhite/everglades/orthomosaics/2022/StartMel/&lt;/span>&lt;span class="si">{flight}&lt;/span>&lt;span class="s2">.tif&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">FLIGHTS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TEST&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">flight&lt;/span> &lt;span class="c1"># ORTHOMOSAICS.flight&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">SITES&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;StartMel&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">FLIGHTS&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># ORTHOMOSAICS.site&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">YEARS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;2022&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">FLIGHTS&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1">#ORTHOMOSAICS.year&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="r---targets">R - targets&lt;/h2></description></item></channel></rss>